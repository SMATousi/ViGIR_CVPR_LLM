{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe550622-b794-4ed7-aaa3-e3ca5fe9c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import signal\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from transformers import AutoProcessor, CLIPModel # pip install transformers\n",
    "# jupyter notebook --ip 0.0.0.0 --port 8889 --allow-root\n",
    "# kubectl port-forward dep-cvpr-7d4d4b94f9-82829 8889:8889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a338ad5-8bed-4509-9023-d7f223cb3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModel, CLIPTokenizer, CLIPModel \n",
    "#from torch.nn import CosineSimilarity\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# cossim = CosineSimilarity(dim=1,eps=1e-6)\n",
    "\n",
    "# def compute_similarity(emb1, emb2):\n",
    "#     emb1  = emb1.unsqueeze(0)\n",
    "#     emb2  = emb2.unsqueeze(0)\n",
    "#     return cossim(emb1,emb2)\n",
    "\n",
    "def get_class_embeddings(prompts, tokenizer, text_encoder):\n",
    "    text_inputs = tokenizer(prompts, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    outputs = text_encoder(**text_inputs)\n",
    "    text_embedding = outputs.pooler_output\n",
    "    return text_embedding\n",
    "    \n",
    "def get_query_embedding(query_prompt, tokenizer, text_encoder):\n",
    "    \n",
    "    query_input = tokenizer(query_prompt, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    query_output = text_encoder(**query_input)\n",
    "    query_embedding = query_output.pooler_output\n",
    "    return query_embedding\n",
    "\n",
    "def compute_scores(class_embeddings, query_embedding, prompts):\n",
    "     # Compute cosine similarity scores\n",
    "    similarity_scores = cosine_similarity(query_embedding, class_embeddings, dim=1)  # Shape: [37]\n",
    "    \n",
    "    # Find the highest matching score and corresponding item\n",
    "    max_score_index = torch.argmax(similarity_scores).item()\n",
    "    max_score = similarity_scores[max_score_index].item()\n",
    "    best_match = prompts[max_score_index]\n",
    "    \n",
    "    # Print the result\n",
    "   # print(f\"Best match: {best_match} with a similarity score of {max_score:.4f}\")\n",
    "    return best_match\n",
    "    \n",
    "    \n",
    "# CLIPText model is the text encoder for clip\n",
    "# CLIPTextModelWithProjection is the text encoder + projection layer \n",
    "# to place the text embeddings in the same embedding space as the image embeddings\n",
    "\n",
    "model_id = \"openai/clip-vit-large-patch14\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id)\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id).to(device)\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "Oxford_pets_prompts = [\"Abyssinian\",\"Bengal\", \"Bombay\", \"Birman\", \"British Shorthair\", \"Maine Coon\", \"Persian\", \"Egyptian Mau\",\n",
    "           \"Ragdoll\", \"Russian Blue\", \"Siamese\", \"Sphynx\", \"Boxer\", \"Keeshond\", \"Havanese\", \"Basset Hound\", \"English Setter\",\n",
    "           \"Miniature Pinscher\", \"Chihuahua\", \"Great Pyrenees\", \"German Shorthaired\", \"Beagle\", \"Staffordshire Bull Terrier\",\n",
    "           \"English Cocker Spaniel\", \"New Found Land\", \"Pomeranian\", \"Leonberger\", \"American Pit Bull Terrier\", \"Wheaten Terrier\",\n",
    "           \"Japanese Chin\", \"Samyod\", \"Samoyed\", \"Samyoed\", \"Scottish Terrier\", \"Shiba Inu\", \"Pug\", \"Saint Bernard\", \"American Bulldog\", \"Yorkshire Terrier\"]\n",
    "\n",
    "Eurosat_prompts = [\"SeaLake\", \"PermanentCrop\", \"River\", \"Residential\", \"Pasture\", \"Industrial\", \"Highway\", \"HerbaceousVegetation\", \n",
    "                   \"Forest\",\"AnnualCrop\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce17b13-7510-480a-bda8-9e8ef1e4d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = [\"A photo of a Samyoed\"]\n",
    "query_embedding = get_query_embedding(query_prompt, tokenizer, text_encoder)\n",
    "compute_scores(text_embedding, query_embedding, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa799b-c3c3-4f84-902d-594adb55d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(emb1, emb2):\n",
    "    emb1  = emb1.unsqueeze(0)\n",
    "    emb2  = emb2.unsqueeze(0)\n",
    "    return cossim(emb1,emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4cd08-5a6d-486b-930e-1459bc1364d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_numbers_Eurosat = {\n",
    "    \"SeaLake\": 9,\n",
    "    \"PermanentCrop\": 6,\n",
    "    \"River\": 8,\n",
    "    \"Residential\": 7,\n",
    "    \"Pasture\": 5,\n",
    "    \"Industrial\": 4,\n",
    "    \"Highway\": 3,\n",
    "    \"HerbaceousVegetation\": 2,\n",
    "    \"Forest\": 1,\n",
    "    \"AnnualCrop\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bcfff-1a29-4d3c-aa65-4892c00311f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_numbers[Eurosat_prompts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fddb1a-5b12-48c6-b91a-292571576259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "save_path = \"/mnt/Software/ViGIR_CVPR_LLM/prompting_framework/results_oxford_pets_new_prompt/\"\n",
    "data_path = '/mnt/Software/ViGIR_CVPR_LLM/prompting_framework/oxford_pets_new_prompt_rebuttal/'\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "# Filter only the .json files\n",
    "prediction_files = [f for f in all_files if f.endswith('.json')]\n",
    "\n",
    "#print(prediction_files)\n",
    "oxford_pet_embeddings = get_class_embeddings(Oxford_pets_prompts, tokenizer, text_encoder)\n",
    "\n",
    "for i in range(len(prediction_files)) : # for all prediction files\n",
    "    if \"minicpm\" not in prediction_files[i] : \n",
    "        print(f\"Skipping {prediction_files[i]}...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing {prediction_files[i]}...\")\n",
    "    oxford_pets_predictions = load_json(os.path.join(data_path,prediction_files[i]))\n",
    "    results_matching = {}\n",
    "    count_long_responses = 0\n",
    "    j = 0\n",
    "    N = len(oxford_pets_predictions.items())\n",
    "    for image_path, label in oxford_pets_predictions.items():\n",
    "        fname = os.path.basename(image_path)\n",
    "        \n",
    "        #print(f\"{j}/{N} | File: {fname}\") #, Label: {label}\")\n",
    "        j+=1\n",
    "        if len(label) > 50 :\n",
    "             query_prompt = label[:50]\n",
    "             count_long_responses+=1\n",
    "        else:\n",
    "             query_prompt = label\n",
    "        query_embedding = get_query_embedding(query_prompt, tokenizer,text_encoder)\n",
    "        matched_label  = compute_scores(oxford_pet_embeddings,query_embedding, Oxford_pets_prompts)\n",
    "        print(f\"{j}/{N} | File: {fname} | Match: {matched_label}, VLM Output:  {label}\")\n",
    "        class_id = class_numbers_oxford_pets[matched_label]\n",
    "        # # print(class_id)\n",
    "        results_matching[fname] = class_id\n",
    "        \n",
    "    \n",
    "    \n",
    "    # json_file_path = os.path.join(save_path, prediction_files[i])\n",
    "    \n",
    "    # # Write the dictionary to the JSON file\n",
    "    # with open(json_file_path, 'w') as f:\n",
    "    #     json.dump(results_matching, f, indent=4)  # 'indent=4' is optional for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b1c25-2a78-4705-b167-f31198f8ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54041856-d6a7-4344-84c1-b6a582c429e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd51ae-e582-46b3-8709-65abc690a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ebd30-abba-4daf-9139-77687a932b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "eurosat_embeddings = get_class_embeddings(Eurosat_prompts, tokenizer, text_encoder)\n",
    "query_prompt = [\"AnnualCrop\"]\n",
    "query_embedding = get_query_embedding(query_prompt, tokenizer,text_encoder)\n",
    "compute_scores(eurosat_embeddings,query_embedding, Eurosat_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1256d4e-b6e5-41b5-9eb2-7e1c7415b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_numbers_oxford_pets = {\"Abyssinian\" : 0,\n",
    "                             \"Bengal\" : 5,\n",
    "                             \"Bombay\" : 7,\n",
    "                             \"Birman\" : 6,\n",
    "                             \"British Shorthair\": 9,\n",
    "                             \"Maine Coon\": 20,\n",
    "                             \"Persian\": 23,\n",
    "                             \"Egyptian Mau\" : 11,\n",
    "                             \"Ragdoll\" : 26,\n",
    "                             \"Russian Blue\" : 27,\n",
    "                             \"Siamese\" : 32,\n",
    "                             \"Sphynx\" : 33,\n",
    "                             \"Boxer\" : 8,\n",
    "                             \"Keeshond\" : 18,\n",
    "                             \"Havanese\" : 16,\n",
    "                             \"Basset Hound\" : 3,\n",
    "                             \"English Setter\" : 13,\n",
    "                             \"Miniature Pinscher\" : 21,\n",
    "                             \"Chihuahua\" : 10,\n",
    "                             \"Great Pyrenees\" : 15,\n",
    "                             \"German Shorthaired\" : 14,\n",
    "                             \"Beagle\" : 4,\n",
    "                             \"Staffordshire Bull Terrier\" : 34,\n",
    "                             \"English Cocker Spaniel\" : 12,\n",
    "                             \"New Found Land\" : 22,\n",
    "                             \"Pomeranian\" : 24,\n",
    "                             \"Leonberger\" : 19,\n",
    "                             \"American Pit Bull Terrier\" : 2,\n",
    "                             \"Wheaten Terrier\" : 35,\n",
    "                             \"Japanese Chin\" : 17,\n",
    "                             \"Samyod\" : 29, \n",
    "                             \"Samoyed\" : 29, \n",
    "                             \"Samyoed\" : 29,\n",
    "                             \"Scottish Terrier\" : 30,\n",
    "                             \"Shiba Inu\" : 31,\n",
    "                             \"Pug\" : 25,\n",
    "                             \"Saint Bernard\" : 28,\n",
    "                             \"American Bulldog\" : 1,\n",
    "                             \"Yorkshire Terrier\" : 36\n",
    "                            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618b60c-dce2-46c5-9c95-aaf86b78ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_numbers_oxford_pets = {\n",
    "    \"abyssinian\": 0,\n",
    "    \"american_bulldog\": 1,\n",
    "    \"american_pit_bull_terrier\": 2,\n",
    "    \"basset_hound\": 3,\n",
    "    \"beagle\": 4,\n",
    "    \"bengal\": 5,\n",
    "    \"birman\": 6,\n",
    "    \"bombay\": 7,\n",
    "    \"boxer\": 8,\n",
    "    \"british_shorthair\": 9,\n",
    "    \"chihuahua\": 10,\n",
    "    \"egyptian_mau\": 11,\n",
    "    \"english_cocker_spaniel\": 12,\n",
    "    \"english_setter\": 13,\n",
    "    \"german_shorthaired\": 14,\n",
    "    \"great_pyrenees\": 15,\n",
    "    \"havanese\": 16,\n",
    "    \"japanese_chin\": 17,\n",
    "    \"keeshond\": 18,\n",
    "    \"leonberger\": 19,\n",
    "    \"maine_coon\": 20,\n",
    "    \"miniature_pinscher\": 21,\n",
    "    \"newfoundland\": 22,\n",
    "    \"persian\": 23,\n",
    "    \"pomeranian\": 24,\n",
    "    \"pug\": 25,\n",
    "    \"ragdoll\": 26,\n",
    "    \"russian_blue\": 27,\n",
    "    \"saint_bernard\": 28,\n",
    "    \"samoyed\": 29,\n",
    "    \"scottish_terrier\": 30,\n",
    "    \"shiba_inu\": 31,\n",
    "    \"siamese\": 32,\n",
    "    \"sphynx\": 33,\n",
    "    \"staffordshire_bull_terrier\": 34,\n",
    "    \"wheaten_terrier\": 35,\n",
    "    \"yorkshire_terrier\": 36\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915b63c-b7fa-455c-95c2-51ef7d99e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Eurosat_prompts = [\"SeaLake\", \"PermanentCrop\",\"River\",\"Residential\",\"Pasture\",\"Industrial\", \"Highway\", \"HerbaceousVegetation\", \"Forest\",\"AnnualCrop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655cbbf1-b20f-4558-9fad-06f731f4752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\"Abyssinian\",\"Bengal\", \"Bombay\", \"Birman\", \"British Shorthair\", \"Maine Coon\", \"Persian\", \"Egyptian Mau\",\n",
    "           \"Ragdoll\", \"Russian Blue\", \"Siamese\", \"Sphynx\", \"Boxer\", \"Keeshond\", \"Havanese\", \"Basset Hound\", \"English Setter\",\n",
    "           \"Miniature Pinscher\", \"Chihuahua\", \"Great Pyrenees\", \"German Shorthaired\", \"Beagle\", \"Staffordshire Bull Terrier\",\n",
    "           \"English Cocker Spaniel\", \"New Found Land\", \"Pomeranian\", \"Leonberger\", \"American Pit Bull Terrier\", \"Wheaten Terrier\",\n",
    "           \"Japanese Chin\", \"Samyod\", \"Scottish Terrier\", \"Shiba Inu\", \"Pug\", \"Saint Bernard\", \"American Bulldog\", \"Yorkshire Terrier\"]\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deeedd4-f365-4cb3-833e-0f57e61d62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/root/home/data/hateful_memes/'\n",
    "images_path = os.path.join(base_path, \"img\")\n",
    "\n",
    "list_of_image_names = os.listdir(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289106a-2b7f-4a3d-ae1f-1539f5bdc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_yes_no(text):\n",
    "    # Strip any leading/trailing whitespace and convert to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # Check if the text starts with 'yes' or 'no'\n",
    "    if text.startswith(\"yes\"):\n",
    "        return 1\n",
    "    elif text.startswith(\"no\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None  \n",
    "    \n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line.strip())\n",
    "            data.append(entry)\n",
    "    return data\n",
    "\n",
    "def load_image_and_label(entry, img_base_path):\n",
    "    img_path = f\"{img_base_path}/{entry['img']}\"\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        label = entry['label']\n",
    "        return img, label\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image {img_path} not found.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def load_dev_file(input_file):\n",
    "    dev_data = {}\n",
    "    with open(input_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            # Load each JSON line as a dictionary\n",
    "            entry = json.loads(line.strip())\n",
    "        \n",
    "            # Use the image path as the key and the label as the value\n",
    "            dev_data[entry[\"img\"]] = entry[\"label\"]\n",
    "    return dev_data\n",
    "\n",
    "dev_file = os.path.join(base_path, \"dev.jsonl\")\n",
    "dev_data = load_dev_file(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba100c3-68e2-4b91-a2bc-6a7b3a1fe339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_models = ['llava:7b', \n",
    "                  'llava:13b',\n",
    "                  'llava:34b',\n",
    "                  'llava-llama3',\n",
    "                  'bakllava',\n",
    "                  'moondream',\n",
    "                  'minicpm-v',\n",
    "                  'llava-phi3']\n",
    "\n",
    "ollama.pull('llava-phi3') #pull the desired model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af583381-6e87-481a-89a0-52441a4ca7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "options= {  # new\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048, # must be set, otherwise slightly random output\n",
    "        }\n",
    "    \n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ed5522-a69b-4d27-93c4-3ad51b8ac795",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 =  \"Here is a list of aircrafts for fine-grained images classification: Abingdon Spherical Free Balloon,  AEG Wagner Eule, Aeris Naviter AN-2 Enara, Aeritalia F-104S Starfighter\" \n",
    "aircraft_context = ollama.generate(model='llava-phi3', prompt=prompt_1, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2d5f20-052b-461b-bbce-4d35cb603036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llava-phi3',\n",
       " 'created_at': '2025-02-12T17:43:11.975921581Z',\n",
       " 'response': \"\\nBased on the information provided, here are some possible classifications for each aircraft:\\n\\n* Abingdon Spherical Free Balloon: This is likely a type of hot air balloon or blimp. It may be used for recreational purposes or scientific research.\\n* AEG Wagner Eule: This appears to be an early German fighter plane from World War I. It was known for its distinctive design and high speed.\\n* Aeris Naviter AN-2 Enara: This is a Soviet-designed transport aircraft that was used during the Cold War era. It was designed for short takeoff and landing capabilities, making it ideal for use in remote areas or austere environments.\\n* Aeritalia F-104S Starfighter: This is an Italian fighter jet that was developed in the 1950s and used by both the Italian Air Force and other countries' air forces. It was known for its high speed, agility, and ability to carry nuclear weapons.\\n\\nIt's worth noting that some of these classifications may be subject to interpretation or debate, as aircraft designations can sometimes be confusing or ambiguous.\",\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'context': [32010,\n",
       "  29871,\n",
       "  13,\n",
       "  10605,\n",
       "  338,\n",
       "  263,\n",
       "  1051,\n",
       "  310,\n",
       "  15780,\n",
       "  29879,\n",
       "  363,\n",
       "  2691,\n",
       "  29899,\n",
       "  3874,\n",
       "  1312,\n",
       "  4558,\n",
       "  12965,\n",
       "  29901,\n",
       "  1976,\n",
       "  292,\n",
       "  9176,\n",
       "  317,\n",
       "  8096,\n",
       "  936,\n",
       "  12362,\n",
       "  7392,\n",
       "  417,\n",
       "  265,\n",
       "  29892,\n",
       "  29871,\n",
       "  319,\n",
       "  11787,\n",
       "  25439,\n",
       "  382,\n",
       "  1297,\n",
       "  29892,\n",
       "  18682,\n",
       "  275,\n",
       "  13132,\n",
       "  1524,\n",
       "  13764,\n",
       "  29899,\n",
       "  29906,\n",
       "  1174,\n",
       "  2518,\n",
       "  29892,\n",
       "  18682,\n",
       "  2410,\n",
       "  423,\n",
       "  383,\n",
       "  29899,\n",
       "  29896,\n",
       "  29900,\n",
       "  29946,\n",
       "  29903,\n",
       "  7828,\n",
       "  1003,\n",
       "  29882,\n",
       "  357,\n",
       "  32007,\n",
       "  29871,\n",
       "  13,\n",
       "  32001,\n",
       "  29871,\n",
       "  13,\n",
       "  13,\n",
       "  29933,\n",
       "  1463,\n",
       "  373,\n",
       "  278,\n",
       "  2472,\n",
       "  4944,\n",
       "  29892,\n",
       "  1244,\n",
       "  526,\n",
       "  777,\n",
       "  1950,\n",
       "  770,\n",
       "  8232,\n",
       "  363,\n",
       "  1269,\n",
       "  15780,\n",
       "  29901,\n",
       "  13,\n",
       "  13,\n",
       "  29930,\n",
       "  1976,\n",
       "  292,\n",
       "  9176,\n",
       "  317,\n",
       "  8096,\n",
       "  936,\n",
       "  12362,\n",
       "  7392,\n",
       "  417,\n",
       "  265,\n",
       "  29901,\n",
       "  910,\n",
       "  338,\n",
       "  5517,\n",
       "  263,\n",
       "  1134,\n",
       "  310,\n",
       "  7375,\n",
       "  4799,\n",
       "  6411,\n",
       "  417,\n",
       "  265,\n",
       "  470,\n",
       "  1999,\n",
       "  6574,\n",
       "  29889,\n",
       "  739,\n",
       "  1122,\n",
       "  367,\n",
       "  1304,\n",
       "  363,\n",
       "  28709,\n",
       "  1288,\n",
       "  11976,\n",
       "  470,\n",
       "  16021,\n",
       "  5925,\n",
       "  29889,\n",
       "  13,\n",
       "  29930,\n",
       "  319,\n",
       "  11787,\n",
       "  25439,\n",
       "  382,\n",
       "  1297,\n",
       "  29901,\n",
       "  910,\n",
       "  5692,\n",
       "  304,\n",
       "  367,\n",
       "  385,\n",
       "  4688,\n",
       "  5332,\n",
       "  285,\n",
       "  14643,\n",
       "  10694,\n",
       "  515,\n",
       "  2787,\n",
       "  3362,\n",
       "  306,\n",
       "  29889,\n",
       "  739,\n",
       "  471,\n",
       "  2998,\n",
       "  363,\n",
       "  967,\n",
       "  8359,\n",
       "  573,\n",
       "  2874,\n",
       "  322,\n",
       "  1880,\n",
       "  6210,\n",
       "  29889,\n",
       "  13,\n",
       "  29930,\n",
       "  18682,\n",
       "  275,\n",
       "  13132,\n",
       "  1524,\n",
       "  13764,\n",
       "  29899,\n",
       "  29906,\n",
       "  1174,\n",
       "  2518,\n",
       "  29901,\n",
       "  910,\n",
       "  338,\n",
       "  263,\n",
       "  15308,\n",
       "  29899,\n",
       "  2783,\n",
       "  12961,\n",
       "  8608,\n",
       "  15780,\n",
       "  393,\n",
       "  471,\n",
       "  1304,\n",
       "  2645,\n",
       "  278,\n",
       "  26731,\n",
       "  3362,\n",
       "  3152,\n",
       "  29889,\n",
       "  739,\n",
       "  471,\n",
       "  8688,\n",
       "  363,\n",
       "  3273,\n",
       "  2125,\n",
       "  2696,\n",
       "  322,\n",
       "  25325,\n",
       "  27108,\n",
       "  29892,\n",
       "  3907,\n",
       "  372,\n",
       "  10839,\n",
       "  363,\n",
       "  671,\n",
       "  297,\n",
       "  7592,\n",
       "  10161,\n",
       "  470,\n",
       "  21996,\n",
       "  406,\n",
       "  23136,\n",
       "  29889,\n",
       "  13,\n",
       "  29930,\n",
       "  18682,\n",
       "  2410,\n",
       "  423,\n",
       "  383,\n",
       "  29899,\n",
       "  29896,\n",
       "  29900,\n",
       "  29946,\n",
       "  29903,\n",
       "  7828,\n",
       "  1003,\n",
       "  29882,\n",
       "  357,\n",
       "  29901,\n",
       "  910,\n",
       "  338,\n",
       "  385,\n",
       "  10545,\n",
       "  285,\n",
       "  14643,\n",
       "  22588,\n",
       "  393,\n",
       "  471,\n",
       "  8906,\n",
       "  297,\n",
       "  278,\n",
       "  29871,\n",
       "  29896,\n",
       "  29929,\n",
       "  29945,\n",
       "  29900,\n",
       "  29879,\n",
       "  322,\n",
       "  1304,\n",
       "  491,\n",
       "  1716,\n",
       "  278,\n",
       "  10545,\n",
       "  5593,\n",
       "  11004,\n",
       "  322,\n",
       "  916,\n",
       "  10916,\n",
       "  29915,\n",
       "  4799,\n",
       "  8249,\n",
       "  29889,\n",
       "  739,\n",
       "  471,\n",
       "  2998,\n",
       "  363,\n",
       "  967,\n",
       "  1880,\n",
       "  6210,\n",
       "  29892,\n",
       "  946,\n",
       "  1793,\n",
       "  29892,\n",
       "  322,\n",
       "  11509,\n",
       "  304,\n",
       "  8677,\n",
       "  20346,\n",
       "  25340,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  3112,\n",
       "  29915,\n",
       "  29879,\n",
       "  7088,\n",
       "  451,\n",
       "  292,\n",
       "  393,\n",
       "  777,\n",
       "  310,\n",
       "  1438,\n",
       "  770,\n",
       "  8232,\n",
       "  1122,\n",
       "  367,\n",
       "  4967,\n",
       "  304,\n",
       "  19854,\n",
       "  470,\n",
       "  27836,\n",
       "  29892,\n",
       "  408,\n",
       "  15780,\n",
       "  2874,\n",
       "  800,\n",
       "  508,\n",
       "  6041,\n",
       "  367,\n",
       "  16051,\n",
       "  470,\n",
       "  22363,\n",
       "  681,\n",
       "  29889],\n",
       " 'total_duration': 4714220336,\n",
       " 'load_duration': 2355727493,\n",
       " 'prompt_eval_count': 66,\n",
       " 'prompt_eval_duration': 185000000,\n",
       " 'eval_count': 250,\n",
       " 'eval_duration': 2102000000}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aircraft_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7647af8d-afd7-4b30-b058-1eae7c67050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = Image.open('fgvc2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ed44ec7-bac8-43db-b16b-850d484f731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applebananacherry\n"
     ]
    }
   ],
   "source": [
    "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
    "list_string = \"\".join(my_list)\n",
    "print(list_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b9c2f79-13a9-40ce-9f9e-89d2438f6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "image_name = 'fgvc2.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a812c53d-a8d9-499f-9ae3-c4610ff589ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"What aircraft category is this? Tell me what categories I have provided in the context.\"\n",
    "aircraft_class = ollama.generate(model='llava-phi3', prompt=prompt_2, images=[os.path.join(base_path,image_name)], options=options, context=aircraft_context['context'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54715ab3-0c17-41b1-9aa7-f0090d8ed0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_embedding(class_names : str, model : str,  options : dict): -> list \n",
    "    \"\"\"\n",
    "        CVPR_W !!!\n",
    "        - Function to create a context embedding for our given fine-grained class names!\n",
    "        - We are interested in supplying this context to the VLM as it works on classifying\n",
    "           images from a fine-grained dataset with numerous classes\n",
    "\n",
    "        Inputs :\n",
    "        -------\n",
    "            class_names : str\n",
    "                comma separated long string of class names.\n",
    "                ex. class_names = \"Honda accord, mazda rx9, mercedes benz c300\"\n",
    "            model       : str\n",
    "                model being used in current experiment. \n",
    "                ex. if using 'llava-llama3' as the current vlm then we need to \n",
    "                use it as well for embedding extraction.\n",
    "            options     : dict\n",
    "                VLM options.\n",
    "                ex. options= {  \n",
    "                            \"seed\": 123,\n",
    "                            \"temperature\": 0,\n",
    "                            \"num_ctx\": 2048, # must be set, otherwise slightly random output\n",
    "                        }\n",
    "                \n",
    "        Output :\n",
    "        --------\n",
    "            context_embedding : List\n",
    "                vlm generated context embedding to aid in informed fine-grained classification.\n",
    "                \n",
    "    \"\"\"\n",
    "    prompt = \"You are working on a difficult fine-grained image classification task with the following classes: \" + class_names \n",
    "    context_response = ollama.generate(model=model, prompt=class_names, options=options)\n",
    "    return context_response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e01ff3ed-8f6f-4d3f-bf49-712da94ca4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llava-phi3', 'created_at': '2025-02-12T20:26:39.282834707Z', 'response': '\\n1. Abyssinian: This breed is known for its athletic build, large ears, and distinctive coat pattern of black and tan or brown and tan patches. They are intelligent, active, and make great companions for those who enjoy outdoor activities.\\n2. Bengal: The Bengal cat has a unique appearance that resembles both a domestic shorthair and a wild feline. They have short, dense fur in various colors such as brown, gray, or black, with white markings on their face and paws. They are playful, energetic, and curious cats who love to explore their surroundings.\\n3. Bombay: The Bombay cat is a small breed that has a sleek, short coat in shades of gray, beige, or brown. They have a muscular build and an alert expression, making them excellent companions for those who enjoy interactive playtime.\\n4. Birman: The Birman cat is known for its long, silky fur in various colors such as blue, cream, or brindle. They are affectionate, gentle, and intelligent cats who love to be around their human family members.\\n5. Maine Coon: This breed of cat has a large body with a bushy tail and tufts of hair on the ears and paws. They have a friendly and social personality, making them great companions for those who enjoy spending time at home or outdoors.\\n6. Persian: The Persian cat is known for its long, luxurious coat in various colors such as white, cream, or gray. They are gentle, affectionate, and make great lap cats for those who enjoy a calm and relaxing environment.\\n7. Egyptian Mau: This breed of cat has a short, sleek coat in shades of black, brown, or gray with white markings on the face and legs. They are energetic, playful, and make great companions for those who enjoy interactive playtime.\\n8. Ragdoll: The Ragdoll is known for its long, silky fur in various colors such as cream, lavender, or white with darker points on the face, paws, and tail. They are affectionate, gentle, and make great companions for those who enjoy a calm and relaxing environment.\\n9. Russian Blue: The Russian Blue cat has a short, dense coat in shades of blue-gray that resembles the color of a sapphire. They are intelligent, curious, and make great companions for those who enjoy interactive playtime.\\n10. Siamese: This breed of cat is known for its sleek, slender body with a short coat in shades of cream or light tan with darker points on the ears, face, paws, and tail. They are affectionate, social, and make great companions for those who enjoy interactive playtime.\\n11. Sphynx: The Sphynx cat is known for its lack of fur, giving it a wrinkled appearance that resembles a lizard or other reptile. They have a friendly personality, making them great companions for those who enjoy interactive playtime.\\n12. Boxer: This breed of dog has a muscular build with short, smooth coats in shades of fawn or brindle. They are affectionate, loyal, and make great companions for families with children.\\n13. Keeshond: The Keeshond is known for its long, curly coat in various colors such as white, black, or gray. They have a friendly personality, making them great companions for those who enjoy interactive playtime.\\n14. English Setter: This breed of dog has a medium-sized build with a short, smooth coat in shades of cream or tan. They are intelligent, energetic, and make great companions for those who enjoy outdoor activities.\\n15. Miniature Pinscher: The Miniature Pinscher is known for its small size and lively personality. They have a short, smooth coat in shades of red or black with white markings on the face and legs. They are affectionate, playful, and make great companions for those who enjoy interactive playtime.\\n16. Chihuahua: The Chihuahua is known for its small size and lively personality. They have a short coat in various colors such as black or brown with white markings on the face and legs. They are affectionate, loyal, and make great companions for those who enjoy interactive playtime.\\n17. Great Pyrenees: This breed of dog is known for its large size and gentle personality. They have a thick coat in shades of white or cream with black markings on the face and legs. They are loyal, protective, and make great companions for those who enjoy outdoor activities.\\n18. German Shorthaired: The German Shorthaired is known for its medium-sized build with a short, smooth coat in shades of tan or cream. They have a friendly personality, making them great companions for families with children.\\n19. Beagle: This breed of dog has a small size and lively personality. They have a short coat in various colors such as white, brown, or black with distinctive markings on the face and legs. They are affectionate, curious, and make great companions for those who enjoy interactive playtime.\\n20. English Cocker Spaniel: The English Cocker Spaniel is known for its medium-sized build with a short, silky coat in shades of red or black. They have a friendly personality, making them great companions for families with children.\\n21. Newfoundland: This breed of dog has a large size and gentle personality. They have a thick coat in various colors such as white, brown, or gray. They are loyal, protective, and make great companions for those who enjoy outdoor activities.\\n22. Pomeranian: The Pomeranian is known for its small size and lively personality. They have a short, dense coat in various colors such as white, black, or tan. They are affectionate, playful, and make great companions for those who enjoy interactive playtime.\\n23. Leonberger: This breed of dog has a large size and gentle personality. They have a thick coat in shades of cream or fawn with distinctive markings on the face and legs. They are loyal, protective, and make great companions for those who enjoy outdoor activities.\\n24. American Pit Bull Terrier: The American Pit Bull Terrier is known for its medium-sret size and lively personality. They have a short, smooth coat in shades of white or fawn with distinctive markings on the face and legs. They are loyal, protective, and make great companions for those who enjoy interactive playtime.\\n25. Wheaten Terrier: The Wheaten Terrier is known for its small size and lively personality. They have a short, dense coat in shades of wheat or light brown with distinctive markings on the face and legs. They are affectionate, playful, and make great companions for those who enjoy interactive playtime.\\n26. Japanese Chin: The Japanese Chin is known for its small size and lively personality. They have a short, silky coat in shades of white or cream with distinctive markings on the face and legs. They are affectionate, gentle, and make great companions for those who enjoy interactive playtime.\\n27. Samoyed: The Samoyed is known for its medium-sized build with a thick coat in various colors such as white, cream, or light brown. They have a friendly personality, making them great companions for families with children.\\n28. Scottish Terrier: This breed of dog has a small size and lively personality. They have a short, dense coat in shades of black or gray with distinctive markings on the face and legs. They are loyal, protective, and make great companions for those who enjoy interactive playtime.\\n29. Pug: The Pug is known for its small size and lively personality. They have a short, smooth coat in shades of fawn or black with distinctive markings on the face and legs. They are affectionate, playful, and make great companions for those who enjoy interactive playtime.\\n30. Saint Bernard: This breed of dog has a large size and gentle personality. They have a thick coat in shades of white or cream with distinctive markings on the face and legs. They are loyal, protective, and make great companions for those who enjoy outdoor activities.', 'done': True, 'done_reason': 'stop', 'context': [32010, 29871, 13, 29909, 1609, 893, 262, 713, 29892, 20781, 284, 29892, 24347, 388, 29892, 29871, 17853, 1171, 1919, 29871, 4908, 1383, 2072, 1466, 1919, 29871, 26160, 3189, 265, 1919, 29871, 9034, 713, 1919, 29871, 12892, 713, 22209, 1919, 390, 351, 29881, 3028, 1919, 29871, 10637, 10924, 1919, 29871, 317, 2829, 968, 1919, 29871, 317, 561, 948, 29916, 1919, 29871, 11773, 261, 1919, 29871, 4813, 12094, 898, 1919, 29871, 21480, 273, 968, 1919, 29871, 17328, 300, 379, 618, 1919, 29871, 4223, 3789, 357, 1919, 29924, 2172, 1535, 17434, 19996, 1919, 29871, 678, 4861, 29884, 801, 3357, 1919, 29871, 7027, 10772, 1267, 12712, 1919, 29871, 5332, 1383, 2072, 29874, 2859, 1919, 29871, 1522, 20860, 1919, 29871, 15351, 536, 11750, 20293, 5061, 4336, 1919, 4223, 315, 8658, 7948, 709, 1919, 29871, 1570, 7460, 3172, 1919, 29871, 349, 12392, 273, 713, 1919, 29871, 10255, 15734, 1919, 29871, 3082, 29244, 20293, 5061, 4336, 1919, 29871, 26286, 2579, 5061, 4336, 1919, 29967, 21419, 968, 678, 262, 1919, 29871, 3685, 29891, 397, 1919, 29871, 17780, 5061, 4336, 1919, 29871, 1383, 16912, 512, 29884, 1919, 29871, 349, 688, 1919, 29871, 4107, 13337, 1919, 29871, 3082, 8313, 430, 468, 1919, 29871, 3088, 11750, 5061, 4336, 32007, 29871, 13, 32001, 29871, 13, 13, 29896, 29889, 319, 1609, 893, 262, 713, 29901, 910, 2078, 287, 338, 2998, 363, 967, 28563, 293, 2048, 29892, 2919, 22827, 29892, 322, 8359, 573, 24296, 4766, 310, 4628, 322, 10345, 470, 17354, 322, 10345, 13261, 267, 29889, 2688, 526, 13052, 296, 29892, 6136, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 714, 17433, 14188, 29889, 13, 29906, 29889, 20781, 284, 29901, 450, 20781, 284, 6635, 756, 263, 5412, 10097, 393, 620, 1590, 793, 1716, 263, 21849, 528, 2072, 1466, 322, 263, 8775, 285, 5570, 29889, 2688, 505, 3273, 29892, 20619, 3261, 297, 5164, 11955, 1316, 408, 17354, 29892, 16749, 29892, 470, 4628, 29892, 411, 4796, 2791, 886, 373, 1009, 3700, 322, 282, 10467, 29889, 2688, 526, 1708, 1319, 29892, 4527, 657, 293, 29892, 322, 12758, 274, 1446, 1058, 5360, 304, 26987, 1009, 8388, 618, 886, 29889, 13, 29941, 29889, 24347, 388, 29901, 450, 24347, 388, 6635, 338, 263, 2319, 2078, 287, 393, 756, 263, 12844, 1416, 29892, 3273, 24296, 297, 528, 3076, 310, 16749, 29892, 367, 2231, 29892, 470, 17354, 29889, 2688, 505, 263, 2301, 16637, 2048, 322, 385, 6655, 4603, 29892, 3907, 963, 15129, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29946, 29889, 17853, 1171, 29901, 450, 17853, 1171, 6635, 338, 2998, 363, 967, 1472, 29892, 4047, 3459, 3261, 297, 5164, 11955, 1316, 408, 7254, 29892, 907, 314, 29892, 470, 1506, 513, 280, 29889, 2688, 526, 22832, 403, 29892, 9914, 29892, 322, 13052, 296, 274, 1446, 1058, 5360, 304, 367, 2820, 1009, 5199, 3942, 5144, 29889, 13, 29945, 29889, 26160, 3189, 265, 29901, 910, 2078, 287, 310, 6635, 756, 263, 2919, 3573, 411, 263, 27089, 29891, 12464, 322, 5291, 615, 29879, 310, 11315, 373, 278, 22827, 322, 282, 10467, 29889, 2688, 505, 263, 19780, 322, 5264, 2022, 2877, 29892, 3907, 963, 2107, 8119, 1080, 363, 1906, 1058, 13389, 805, 2548, 931, 472, 3271, 470, 714, 1867, 943, 29889, 13, 29953, 29889, 9034, 713, 29901, 450, 9034, 713, 6635, 338, 2998, 363, 967, 1472, 29892, 21684, 332, 2738, 24296, 297, 5164, 11955, 1316, 408, 4796, 29892, 907, 314, 29892, 470, 16749, 29889, 2688, 526, 9914, 29892, 22832, 403, 29892, 322, 1207, 2107, 23012, 274, 1446, 363, 1906, 1058, 13389, 263, 21732, 322, 26681, 292, 5177, 29889, 13, 29955, 29889, 12892, 713, 22209, 29901, 910, 2078, 287, 310, 6635, 756, 263, 3273, 29892, 12844, 1416, 24296, 297, 528, 3076, 310, 4628, 29892, 17354, 29892, 470, 16749, 411, 4796, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 4527, 657, 293, 29892, 1708, 1319, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29947, 29889, 390, 351, 29881, 3028, 29901, 450, 390, 351, 29881, 3028, 338, 2998, 363, 967, 1472, 29892, 4047, 3459, 3261, 297, 5164, 11955, 1316, 408, 907, 314, 29892, 22181, 1581, 29892, 470, 4796, 411, 6501, 261, 3291, 373, 278, 3700, 29892, 282, 10467, 29892, 322, 12464, 29889, 2688, 526, 22832, 403, 29892, 9914, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 263, 21732, 322, 26681, 292, 5177, 29889, 13, 29929, 29889, 10637, 10924, 29901, 450, 10637, 10924, 6635, 756, 263, 3273, 29892, 20619, 24296, 297, 528, 3076, 310, 7254, 29899, 21012, 393, 620, 1590, 793, 278, 2927, 310, 263, 872, 407, 14812, 29889, 2688, 526, 13052, 296, 29892, 12758, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29896, 29900, 29889, 317, 2829, 968, 29901, 910, 2078, 287, 310, 6635, 338, 2998, 363, 967, 12844, 1416, 29892, 2243, 1581, 3573, 411, 263, 3273, 24296, 297, 528, 3076, 310, 907, 314, 470, 3578, 10345, 411, 6501, 261, 3291, 373, 278, 22827, 29892, 3700, 29892, 282, 10467, 29892, 322, 12464, 29889, 2688, 526, 22832, 403, 29892, 5264, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29896, 29896, 29889, 317, 561, 948, 29916, 29901, 450, 317, 561, 948, 29916, 6635, 338, 2998, 363, 967, 10225, 310, 3261, 29892, 6820, 372, 263, 2358, 682, 839, 10097, 393, 620, 1590, 793, 263, 301, 17909, 470, 916, 337, 415, 488, 29889, 2688, 505, 263, 19780, 2022, 2877, 29892, 3907, 963, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29896, 29906, 29889, 11773, 261, 29901, 910, 2078, 287, 310, 11203, 756, 263, 2301, 16637, 2048, 411, 3273, 29892, 10597, 1302, 1446, 297, 528, 3076, 310, 2258, 1233, 470, 1506, 513, 280, 29889, 2688, 526, 22832, 403, 29892, 28108, 29892, 322, 1207, 2107, 8119, 1080, 363, 13175, 411, 4344, 29889, 13, 29896, 29941, 29889, 4813, 12094, 898, 29901, 450, 4813, 12094, 898, 338, 2998, 363, 967, 1472, 29892, 3151, 368, 24296, 297, 5164, 11955, 1316, 408, 4796, 29892, 4628, 29892, 470, 16749, 29889, 2688, 505, 263, 19780, 2022, 2877, 29892, 3907, 963, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29896, 29946, 29889, 4223, 3789, 357, 29901, 910, 2078, 287, 310, 11203, 756, 263, 18350, 29899, 29879, 1891, 2048, 411, 263, 3273, 29892, 10597, 24296, 297, 528, 3076, 310, 907, 314, 470, 10345, 29889, 2688, 526, 13052, 296, 29892, 4527, 657, 293, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 714, 17433, 14188, 29889, 13, 29896, 29945, 29889, 341, 2172, 1535, 17434, 19996, 29901, 450, 341, 2172, 1535, 17434, 19996, 338, 2998, 363, 967, 2319, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 29892, 10597, 24296, 297, 528, 3076, 310, 2654, 470, 4628, 411, 4796, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 22832, 403, 29892, 1708, 1319, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29896, 29953, 29889, 678, 4861, 29884, 801, 3357, 29901, 450, 678, 4861, 29884, 801, 3357, 338, 2998, 363, 967, 2319, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 24296, 297, 5164, 11955, 1316, 408, 4628, 470, 17354, 411, 4796, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 22832, 403, 29892, 28108, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29896, 29955, 29889, 7027, 10772, 1267, 12712, 29901, 910, 2078, 287, 310, 11203, 338, 2998, 363, 967, 2919, 2159, 322, 9914, 2022, 2877, 29889, 2688, 505, 263, 12003, 24296, 297, 528, 3076, 310, 4796, 470, 907, 314, 411, 4628, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 28108, 29892, 12566, 573, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 714, 17433, 14188, 29889, 13, 29896, 29947, 29889, 5332, 1383, 2072, 29874, 2859, 29901, 450, 5332, 1383, 2072, 29874, 2859, 338, 2998, 363, 967, 18350, 29899, 29879, 1891, 2048, 411, 263, 3273, 29892, 10597, 24296, 297, 528, 3076, 310, 10345, 470, 907, 314, 29889, 2688, 505, 263, 19780, 2022, 2877, 29892, 3907, 963, 2107, 8119, 1080, 363, 13175, 411, 4344, 29889, 13, 29896, 29929, 29889, 1522, 20860, 29901, 910, 2078, 287, 310, 11203, 756, 263, 2319, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 24296, 297, 5164, 11955, 1316, 408, 4796, 29892, 17354, 29892, 470, 4628, 411, 8359, 573, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 22832, 403, 29892, 12758, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29906, 29900, 29889, 4223, 315, 8658, 7948, 709, 29901, 450, 4223, 315, 8658, 7948, 709, 338, 2998, 363, 967, 18350, 29899, 29879, 1891, 2048, 411, 263, 3273, 29892, 4047, 3459, 24296, 297, 528, 3076, 310, 2654, 470, 4628, 29889, 2688, 505, 263, 19780, 2022, 2877, 29892, 3907, 963, 2107, 8119, 1080, 363, 13175, 411, 4344, 29889, 13, 29906, 29896, 29889, 1570, 11940, 1049, 29901, 910, 2078, 287, 310, 11203, 756, 263, 2919, 2159, 322, 9914, 2022, 2877, 29889, 2688, 505, 263, 12003, 24296, 297, 5164, 11955, 1316, 408, 4796, 29892, 17354, 29892, 470, 16749, 29889, 2688, 526, 28108, 29892, 12566, 573, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 714, 17433, 14188, 29889, 13, 29906, 29906, 29889, 349, 12392, 273, 713, 29901, 450, 349, 12392, 273, 713, 338, 2998, 363, 967, 2319, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 29892, 20619, 24296, 297, 5164, 11955, 1316, 408, 4796, 29892, 4628, 29892, 470, 10345, 29889, 2688, 526, 22832, 403, 29892, 1708, 1319, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29906, 29941, 29889, 10255, 15734, 29901, 910, 2078, 287, 310, 11203, 756, 263, 2919, 2159, 322, 9914, 2022, 2877, 29889, 2688, 505, 263, 12003, 24296, 297, 528, 3076, 310, 907, 314, 470, 2258, 1233, 411, 8359, 573, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 28108, 29892, 12566, 573, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 714, 17433, 14188, 29889, 13, 29906, 29946, 29889, 3082, 29244, 20293, 5061, 4336, 29901, 450, 3082, 29244, 20293, 5061, 4336, 338, 2998, 363, 967, 18350, 29899, 29879, 2267, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 29892, 10597, 24296, 297, 528, 3076, 310, 4796, 470, 2258, 1233, 411, 8359, 573, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 28108, 29892, 12566, 573, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29906, 29945, 29889, 26286, 2579, 5061, 4336, 29901, 450, 26286, 2579, 5061, 4336, 338, 2998, 363, 967, 2319, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 29892, 20619, 24296, 297, 528, 3076, 310, 21266, 271, 470, 3578, 17354, 411, 8359, 573, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 22832, 403, 29892, 1708, 1319, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29906, 29953, 29889, 10369, 678, 262, 29901, 450, 10369, 678, 262, 338, 2998, 363, 967, 2319, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 29892, 4047, 3459, 24296, 297, 528, 3076, 310, 4796, 470, 907, 314, 411, 8359, 573, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 22832, 403, 29892, 9914, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29906, 29955, 29889, 3685, 12602, 287, 29901, 450, 3685, 12602, 287, 338, 2998, 363, 967, 18350, 29899, 29879, 1891, 2048, 411, 263, 12003, 24296, 297, 5164, 11955, 1316, 408, 4796, 29892, 907, 314, 29892, 470, 3578, 17354, 29889, 2688, 505, 263, 19780, 2022, 2877, 29892, 3907, 963, 2107, 8119, 1080, 363, 13175, 411, 4344, 29889, 13, 29906, 29947, 29889, 17780, 5061, 4336, 29901, 910, 2078, 287, 310, 11203, 756, 263, 2319, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 29892, 20619, 24296, 297, 528, 3076, 310, 4628, 470, 16749, 411, 8359, 573, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 28108, 29892, 12566, 573, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29906, 29929, 29889, 349, 688, 29901, 450, 349, 688, 338, 2998, 363, 967, 2319, 2159, 322, 301, 3598, 2022, 2877, 29889, 2688, 505, 263, 3273, 29892, 10597, 24296, 297, 528, 3076, 310, 2258, 1233, 470, 4628, 411, 8359, 573, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 22832, 403, 29892, 1708, 1319, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 28923, 1708, 2230, 29889, 13, 29941, 29900, 29889, 4107, 13337, 29901, 910, 2078, 287, 310, 11203, 756, 263, 2919, 2159, 322, 9914, 2022, 2877, 29889, 2688, 505, 263, 12003, 24296, 297, 528, 3076, 310, 4796, 470, 907, 314, 411, 8359, 573, 2791, 886, 373, 278, 3700, 322, 21152, 29889, 2688, 526, 28108, 29892, 12566, 573, 29892, 322, 1207, 2107, 8119, 1080, 363, 1906, 1058, 13389, 714, 17433, 14188, 29889], 'total_duration': 21155415034, 'load_duration': 2141368538, 'prompt_eval_count': 204, 'prompt_eval_duration': 145000000, 'eval_count': 1925, 'eval_duration': 18809000000}\n"
     ]
    }
   ],
   "source": [
    "class_names = \"Abyssinian, Bengal, Bombay,  Birman ,  British Shorthair ,  Maine Coon ,  Persian ,  Egyptian Mau , Ragdoll ,  Russian Blue ,  Siamese ,  Sphynx ,  Boxer ,  Keeshond ,  Havanese ,  Basset Hound ,  English Setter ,Miniature Pinscher ,  Chihuahua ,  Great Pyrenees ,  German Shorthaired ,  Beagle ,  Staffordshire Bull Terrier , English Cocker Spaniel ,  New Found Land ,  Pomeranian ,  Leonberger ,  American Pit Bull Terrier ,  Wheaten Terrier ,Japanese Chin ,  Samyod ,  Scottish Terrier ,  Shiba Inu ,  Pug ,  Saint Bernard ,  American Bulldog ,  Yorkshire Terrier\"\n",
    "prompt = \"You are working on a difficult fine-grained image classification task, here are the only classes you can choose from: \" + class_names \n",
    "context_response = ollama.generate(model='llava-phi3', prompt=class_names, options=options)\n",
    "print(context_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49fd70d0-0bdb-48fa-9fa4-16114af0612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2127\n"
     ]
    }
   ],
   "source": [
    "print(len(context_response['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5215409-caf6-439c-9808-9c23c95d4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7385a-8e86-4a68-88be-06c209653eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dev_file\n",
    "hateful_data = {}\n",
    "nonhateful_data = {}\n",
    "\n",
    "for key in dev_data :\n",
    "    if dev_data[key] == 1 :\n",
    "        hateful_data[key] = dev_data[key]\n",
    "    else  :\n",
    "        nonhateful_data[key] = dev_data[key]\n",
    "hateful_images = list(hateful_data.keys())\n",
    "nonhateful_images = list(nonhateful_data.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0374d59-b319-4b02-bc6e-1b0e686499e9",
   "metadata": {},
   "source": [
    "# Prepare CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe9165-a6cf-4d93-9c7f-97235a5e5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "device = \"cuda\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a0f36f-77de-4f4f-9ecf-6a533cfaae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb1 = hateful_embeddings['img/08291.png']\n",
    "# emb1_name = 'img/08291.png'\n",
    "#print(len(hateful_embeddings))\n",
    "emb_path='/mnt/Software/ViGIR_CVPR_LLM/prompting_framework/hateful_memes_embeddings'\n",
    "#torch.save(hateful_embeddings, os.path.join(emb_path,'hateful_embeddings.pth'))\n",
    "#torch.save(nonhateful_embeddings, os.path.join(emb_path,'nonhateful_embeddings.pth'))\n",
    "lhe = torch.load(os.path.join(emb_path,'hateful_embeddings.pth'))\n",
    "lnhe = torch.load(os.path.join(emb_path,'nonhateful_embeddings.pth'))\n",
    "all_embeddings = lhe | lnhe\n",
    "# print(len(all_embeddings))\n",
    "def retrieve_similar(query_embedding, query_image_name, embeddings_dict):\n",
    "    max_cosine_similarity = -float('inf')\n",
    "    closest_image=None\n",
    "    keys = list(embeddings_dict.keys())\n",
    "    for key in keys:\n",
    "        if query_image_name != key :\n",
    "            current_embedding = embeddings_dict[key]\n",
    "            # Normalize embeddings\n",
    "            current_embedding = current_embedding/current_embedding.norm(p=2,dim=-1,keepdim=True)\n",
    "            query_embedding = query_embedding/query_embedding.norm(p=2,dim=-1,keepdim=True)\n",
    "            \n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(current_embedding, query_embedding, dim=1)\n",
    "            #print(cosine_similarity)\n",
    "            if cosine_similarity > max_cosine_similarity :\n",
    "                max_cosine_similarity = cosine_similarity\n",
    "                closest_image = key\n",
    "                #print(f\"new max, {closest_image}, {cosine_similarity}\")\n",
    "        \n",
    "    \n",
    "    return closest_image\n",
    "# print(x)\n",
    "#x = retrieve_similar(emb1, emb1_name, lnhe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609538d-2352-4f32-915f-6331e3a86380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578ed52-768d-48e6-b5c5-6e76545903cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hateful_embeddings['img/08291.png'].shape\n",
    "print(len(hateful_embeddings))\n",
    "emb_path='/mnt/Software/ViGIR_CVPR_LLM/prompting_framework/hateful_memes_embeddings'\n",
    "#torch.save(hateful_embeddings, os.path.join(emb_path,'hateful_embeddings.pth'))\n",
    "#torch.save(nonhateful_embeddings, os.path.join(emb_path,'nonhateful_embeddings.pth'))\n",
    "lhe = torch.load(os.path.join(emb_path,'hateful_embeddings.pth'))\n",
    "lnhe = torch.load(os.path.join(emb_path,'nonhateful_embeddings.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af94eaa-0949-4347-b6d0-4445c18b2ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea82691-a1af-4318-a059-2aa702cf7c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5cb034-f01d-4668-b386-353121d90844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128db12-de0d-43f9-922d-23491608362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a21bd6-07b7-4d5b-8e21-9a5d7ee46f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hateful_embeddings = {}\n",
    "nonhateful_embeddings = {}\n",
    "for image in tqdm(hateful_images) :\n",
    "    img_file = os.path.join(base_path, image)\n",
    "    print(f\"Processing Hateful: {img_file}\")\n",
    "    img = Image.open(img_file)\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    # Move embeddings to CPU\n",
    "    hateful_embeddings[image]=image_features.cpu()\n",
    "    del inputs\n",
    "\n",
    "for image in tqdm(nonhateful_images) :\n",
    "    img_file = os.path.join(base_path, image)\n",
    "    print(f\"Processing Non-Hateful: {img_file}\")\n",
    "    img = Image.open(img_file) \n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    nonhateful_embeddings[image]=image_features.cpu()\n",
    "    del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874031a5-463a-4aa3-88e4-77de371c6c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "options= {  # new\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048, # must be set, otherwise slightly random output\n",
    "        }\n",
    "    \n",
    "llava_7b_emb_labels = {}\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "timeout_duration = 20 # 20 seconds\n",
    "print(f\"Handling the timeout exceptions with timeout duration of {timeout_duration} seconds\")\n",
    "\n",
    "#for image_name in tqdm(list_of_image_names):\n",
    "for key, value in dev_data.items():\n",
    "    print(f\"Image: {key}, Label: {value}\")\n",
    "    # Extract current image name\n",
    "    query_img = key\n",
    "    # Extract its embedding \n",
    "    query_emb = all_embeddings[key]\n",
    "\n",
    "    # Calculate its \"nearest neighbor\" in hateful and non-hateful -- Ramy\n",
    "    similar_hateful = retrieve_similar(query_emb, query_img, lhe)\n",
    "    similar_nonhateful = retrieve_similar(query_emb, query_img, lnhe)\n",
    "    \n",
    "    #random_image_index = np.random.randint(0, len(dev_data))\n",
    "    #random_image_path = os.path.join(base_path, images_list[random_image_index])\n",
    "    \n",
    "    #image_path = os.path.join(base_path, key)  \n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(timeout_duration)  # Set the timeout\n",
    "    \n",
    "    prompt_1 = \"This is  an offensive meme.\"\n",
    "    prompt_2 = \"This is not an offensive meme.\"\n",
    "    prompt_3 = \"How about this one? (Answer only Yes or No)\"\n",
    "    \n",
    "    try:\n",
    "        response_1 = ollama.generate(model='llava:7b', prompt=prompt_1, images=[os.path.join(base_path,similar_hateful)], options=options)\n",
    "        response_2 = ollama.generate(model='llava:7b', prompt=prompt_2, images=[os.path.join(base_path,similar_nonhateful)], options=options, context=response_1['context'])\n",
    "        response_3 = ollama.generate(model='llava:7b', prompt=prompt_3, images=[os.path.join(base_path,query_img)], options=options, context=response_2['context'])\n",
    "        label = check_yes_no(response_3['response'])\n",
    "    except TimeoutException:\n",
    "        print(f\"Prompt for {image_name} took longer than {timeout_duration} seconds. Moving to the next one.\")\n",
    "        label = None\n",
    "\n",
    "    finally:\n",
    "        signal.alarm(0)  # Disable the alarm\n",
    "\n",
    "    llava_7b_emb_labels[query_img] = label\n",
    "    \n",
    "    print(f\"model results -- Image: {query_img}, Label: {label}\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "   "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fdc09eb-61d3-40dd-92c9-b332260d59ac",
   "metadata": {},
   "source": [
    "# Run prompting with image embeddings from CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17159a-ba52-4990-b002-d194e7816ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b0915-dd3e-4d17-8ca6-cf8ebe4ca2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = '/mnt/Context_testing/'\n",
    "im0 = os.path.join(dpath, '6.jpg')\n",
    "im12 = os.path.join(dpath, '5.jpg')\n",
    "imz = os.path.join(dpath, '4.jpg')\n",
    "im3 = os.path.join(dpath, '3.jpg')\n",
    "imp2 = os.path.join(dpath, '2.jpg')\n",
    "imp1 = os.path.join(dpath, '1.jpg')\n",
    "# 6 -0\n",
    "# 5 - 12\n",
    "# 4 - z\n",
    "# 3 - 3\n",
    "# 2 - P\n",
    "# 1 - P\n",
    "#plt.imshow(im1)\n",
    "#plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20e884-7863-45e1-9db9-2c329fc8706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collage(images, grid_rows, grid_cols, padding=10):\n",
    "\n",
    "    img = Image.open(images[0])\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "\n",
    "    # Compute collage dimensions based on grid size provided\n",
    "    collage_width = grid_cols * img_width + (grid_cols + 1) * padding\n",
    "    collage_height = grid_rows * img_height + (grid_rows + 1) * padding\n",
    "\n",
    "\n",
    "    # create empty collage\n",
    "    collage = Image.new('RGB', (collage_width, collage_height), 'white')\n",
    "\n",
    "    # Add the images onto the empty collage we created!\n",
    "    for i, img_path in enumerate(images):\n",
    "        img = Image.open(img_path).resize((img_width, img_height), Image.ANTIALIAS)\n",
    "        x = (i % grid_cols) * (img_width + padding) + padding\n",
    "        y = (i // grid_cols) * (img_height + padding) + padding\n",
    "        collage.paste(img, (x,y))\n",
    "\n",
    "    return collage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395abdf5-d891-4eaa-96e0-16a2529ce6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [im0,im12,imz,imp1,imp2,im3]\n",
    "collage = create_collage(images, 3,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44472305-0c02-4163-b040-8e58b6ac67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(collage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3c9e1-2ede-436a-90a5-beefdd91c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "collage_file = '/mnt/Context_testing/collage.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125fca2-f228-4b95-a132-6d53a1c6f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='llava:7b'\n",
    "#ollama.pull(model) #pull the desired model\n",
    "np.random.seed(0)\n",
    "prompt = \" what specific numbers and letters do you see in this image? There are 6 total\"\n",
    "response_1 = ollama.generate(model=model, prompt=prompt, images=[collage_file], options=options)\n",
    "print(f\"prompt1: {response_1['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f38613-b874-42bc-a78e-a841be006920",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull('llava-phi3') #pull the desired model\n",
    "model='llava-phi3'\n",
    "options= {  # new\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048, # must be set, otherwise slightly random output\n",
    "        }\n",
    "    \n",
    "llava_7b_emb_labels = {}\n",
    "\n",
    "np.random.seed(0)\n",
    "prompt = \"Very briefly, what character do you see in this image?\"\n",
    "response_1 = ollama.generate(model=model, prompt=prompt, images=[im0], options=options)\n",
    "response_2 = ollama.generate(model=model, prompt=prompt, images=[im12], options=options, context=response_1['context'])\n",
    "response_3 = ollama.generate(model=model, prompt=prompt, images=[imz], options=options, context=response_2['context'])\n",
    "response_4 = ollama.generate(model=model, prompt=prompt, images=[im3], options=options, context=response_3['context'])\n",
    "response_5 = ollama.generate(model=model, prompt=prompt, images=[imp2], options=options, context=response_4['context'])\n",
    "response_6 = ollama.generate(model=model, prompt=prompt, images=[imp1], options=options, context=response_5['context'])\n",
    "\n",
    "print(f\"prompt1: {response_1['response']}\")\n",
    "print(f\"prompt2: {response_2['response']}\")\n",
    "print(f\"prompt3: {response_3['response']}\")\n",
    "print(f\"prompt4: {response_4['response']}\")\n",
    "print(f\"prompt5: {response_5['response']}\")\n",
    "print(f\"prompt6: {response_6['response']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce9e2a-bb4f-4624-b5f8-90fabc0fa885",
   "metadata": {},
   "outputs": [],
   "source": [
    "options= {  # new\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048, # must be set, otherwise slightly random output\n",
    "        }\n",
    "    \n",
    "llava_7b_emb_labels = {}\n",
    "\n",
    "np.random.seed(0)\n",
    "prompt = \"What do you see in this image?\"\n",
    "response_1 = ollama.generate(model=model, prompt=prompt, images=[im0], options=options)\n",
    "response_2 = ollama.generate(model=model, prompt=prompt, images=[im12], options=options)#, context=response_1['context'])\n",
    "response_3 = ollama.generate(model=model, prompt=prompt, images=[imz], options=options)#, context=response_2['context'])\n",
    "response_4 = ollama.generate(model=model, prompt=prompt, images=[im3], options=options)#, context=response_3['context'])\n",
    "response_5 = ollama.generate(model=model, prompt=prompt, images=[imp2], options=options)#, context=response_4['context'])\n",
    "response_6 = ollama.generate(model=model, prompt=prompt, images=[imp1], options=options)#, context=response_5['context'])\n",
    "\n",
    "print(response_1['response'])\n",
    "print(response_2['response'])\n",
    "print(response_3['response'])\n",
    "print(response_4['response'])\n",
    "print(response_5['response'])\n",
    "print(response_6['response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12f129-f6d3-4a71-8424-f0fdf2378cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "# Setup options and random seed\n",
    "options = {\n",
    "    \"seed\": 123,\n",
    "    \"temperature\": 0,\n",
    "    \"num_ctx\": 2048,\n",
    "}\n",
    "np.random.seed(0)\n",
    "timeout_duration = 600  # 20 seconds\n",
    "\n",
    "# Initialize the labels dictionary\n",
    "llava_7b_emb_labels = {}\n",
    "\n",
    "# Assign the signal handler for the timeout\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "print(f\"Handling the timeout exceptions with a timeout duration of {timeout_duration} seconds\")\n",
    "\n",
    "for query_img, value in tqdm(dev_data.items()):\n",
    "    print(f\"Image: {query_img}, Label: {value}\")\n",
    "\n",
    "    # Extract embedding\n",
    "    query_emb = all_embeddings[query_img]\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    similar_hateful = retrieve_similar(query_emb, query_img, lhe)\n",
    "    similar_nonhateful = retrieve_similar(query_emb, query_img, lnhe)\n",
    "\n",
    "    # Prompts\n",
    "    prompt_1 = \"This is a meme image. The combination of its text and image content is offensive/ hateful. Learn from this.\"\n",
    "    prompt_2 = \"This is a meme image. The combination of its text and image content is not offensive/ hateful. Learn from this.\"\n",
    "    prompt_3 = \"Is this a hateful meme? (Answer only Yes or No)\"\n",
    "\n",
    "    # Set the alarm for timeout\n",
    "    signal.alarm(timeout_duration)\n",
    "    try:\n",
    "        response_1 = ollama.generate(\n",
    "            model='llava:7b', prompt=prompt_1, images=[os.path.join(base_path, similar_hateful)], options=options\n",
    "        )\n",
    "        if 'context' not in response_1 :\n",
    "            label = None\n",
    "            llava_7b_emb_labels[query_img] = label\n",
    "            continue\n",
    "        response_2 = ollama.generate(\n",
    "            model='llava:7b', prompt=prompt_2, images=[os.path.join(base_path, similar_nonhateful)], options=options, context=response_1['context']\n",
    "        )\n",
    "        if 'context' not in response_2 :\n",
    "            label = None\n",
    "            llava_7b_emb_labels[query_img] = label\n",
    "            continue\n",
    "        response_3 = ollama.generate(\n",
    "            model='llava:7b', prompt=prompt_3, images=[os.path.join(base_path, query_img)], options=options, context=response_2['context']\n",
    "        )\n",
    "        if 'context' not in response_3 :\n",
    "            label = None\n",
    "            llava_7b_emb_labels[query_img] = label\n",
    "            continue\n",
    "            \n",
    "        label = check_yes_no(response_3['response'])\n",
    "    except TimeoutException:\n",
    "        print(f\"Prompt for {query_img} took longer than {timeout_duration} seconds. Moving to the next one.\")\n",
    "        label = None\n",
    "    finally:\n",
    "        signal.alarm(0)  # Disable the alarm\n",
    "\n",
    "    # Store the result\n",
    "    llava_7b_emb_labels[query_img] = label\n",
    "    \n",
    "    print(f\"Model results -- Image: {query_img}, Label: {label}\")\n",
    "    print(\"------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b19d2-6880-4be8-b7e1-d44971b287b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llava_7b_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b0eac-b6f4-45bb-9dfa-2bce82e417fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'context' not in response_2:\n",
    "    print(response_2['response'])\n",
    "    print(\"no context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e4753-7047-47b4-ac6a-a4f7d2d45a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt_x = \"How about this one?(Answer yes or no only)\"\n",
    "response_3 = ollama.generate(model='llava:7b', prompt=prompt_x, images=[os.path.join(base_path, \"img/28017.png\")], options=options, context=response_2['context'])\n",
    "response_3['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef8a17-8800-49df-9482-e075fa617e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_3['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b75ac-d399-4409-8b67-137aee285558",
   "metadata": {},
   "outputs": [],
   "source": [
    "img  = Image.open(os.path.join(base_path, \"img/28017.png\"))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab73a4-0f99-4d7a-95d1-7911f3d14083",
   "metadata": {},
   "outputs": [],
   "source": [
    "img  = Image.open(os.path.join(base_path, similar_hateful))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31144eff-9787-4945-bc5c-8b3993b5babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "img  = Image.open(os.path.join(base_path, similar_nonhateful))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a51c9-2f9d-4381-9ad4-0fd299c83525",
   "metadata": {},
   "outputs": [],
   "source": [
    "img  = Image.open(os.path.join(base_path, query_img))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7472a-117d-458a-8990-feb1f9fef968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_1['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf15623-c542-463a-ac3f-f9cd1d163a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_2['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ea98d-1df8-4953-a320-a2aeb18fbbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3 | two prompts\n",
    "\n",
    "options= {  # new\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048, # must be set, otherwise slightly random output\n",
    "        }\n",
    "    \n",
    "labels_2context_dict = {}\n",
    "\n",
    "np.random.seed(0)\n",
    "for model in list_of_models :\n",
    "    for entry in tqdm(dev_data):\n",
    "        rand_hateful_index = np.random.randint(0, len(gt_hateful))\n",
    "        rand_not_hateful_index = np.random.randint(0, len(gt_not_hateful))\n",
    "        \n",
    "        hateful_path = os.path.join(base_path, \"img\", gt_hateful[rand_hateful_index])\n",
    "        not_hateful_path = os.path.join(base_path, \"img\", gt_not_hateful[rand_not_hateful_index])\n",
    "        \n",
    "        image_path = os.path.join(base_path, entry['img'])\n",
    "        \n",
    "        prompt_1 = \"This is a offensive meme.\"\n",
    "        prompt_2 = \"This is not a offensive meme. \"\n",
    "        prompt_3 = \"Based on the two previous prompts. Is this an offensive meme? answer either yes or no?. \"\n",
    "    \n",
    "        \n",
    "        response_1 = ollama.generate(model=model, prompt=prompt_1, images=[hateful_path], options=options)\n",
    "        response_2 = ollama.generate(model=model, prompt=prompt_2, images=[not_hateful_path], options=options, context=response_1['context'])\n",
    "        response_3 = ollama.generate(model=model, prompt=prompt_3, images=[image_path], options=options, context=response_2['context'])\n",
    "    \n",
    "        label_2context = check_yes_no(response_3['response'])\n",
    "    \n",
    "        image_name =  os.path.basename(entry['img'])\n",
    "        print(label_2context, '--', entry['label'], '--', entry['img'], '--', image_name)\n",
    "    \n",
    "        labels_2context_dict[image_name] = label_2context\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e20bd-75a7-4bb4-8c3f-346212f06b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(\"/mnt\", \"llava7b_emb_3ctx.csv\")\n",
    "print(\"Results on 365 Images (73%) before crashing\")\n",
    "print(\"Prompts:\")\n",
    "print( \"This is a meme image. The combination of its text and image content is offensive/ hateful. Learn from this.\\nThis is a meme image. The combination of its text and image content is not offensive/ hateful. Learn from this.\\nHow about this one? (Answer only Yes or No)\")\n",
    "metrics = compute_metrics(dev_data, llava_7b_emb_labels, \"llava7b\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a556f-6c0e-4693-a481-9cc73c6a634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(gt, predictions, name, output_file=\"metrics.csv\"):\n",
    "    # Ensure we only evaluate on common keys\n",
    "    common_keys = set(gt.keys()).intersection(predictions.keys())\n",
    "    \n",
    "    # Extract lists of labels based on the common keys, filtering out None values\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for key in common_keys:\n",
    "        pred_label = predictions[key]\n",
    "        if pred_label is not None:\n",
    "            y_true.append(gt[key])\n",
    "            y_pred.append(pred_label)\n",
    "    \n",
    "    # Check if there are valid entries left after filtering\n",
    "    if y_true and y_pred:\n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Create a DataFrame to store the results\n",
    "        metrics_df = pd.DataFrame({\n",
    "            \"Model\": [name],\n",
    "            \"Precision\": [precision],\n",
    "            \"Recall\": [recall],\n",
    "            \"F1 Score\": [f1],\n",
    "            \"Accuracy\": [accuracy]\n",
    "        })\n",
    "        \n",
    "        # Display the table\n",
    "        print(metrics_df)\n",
    "        \n",
    "        # Save to a file (append if file already exists)\n",
    "        with open(output_file, \"a\") as f:\n",
    "            metrics_df.to_csv(f, index=False, header=f.tell()==0)\n",
    "    else:\n",
    "        print(f\"No valid entries to compute metrics for {name}\")\n",
    "\n",
    "# Example usage\n",
    "# compute_metrics(gt_dict, predictions_dict, 'Model Metrics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31c55e-5e0e-41eb-8350-5cb57f92bf7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
