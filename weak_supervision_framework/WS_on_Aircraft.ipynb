{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac698f8-3007-4c47-b3b6-85b20184afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ccf6568-d55f-40ed-a15b-f3f9fc0e0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f97eddcc-a490-4296-8181-91357785a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/aircraft/'\n",
    "    llava_7b_results = 'aircraft-llava-7b.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name][\"label\"] if data[image_name][\"label\"] is not None else -1\n",
    "\n",
    "# @labeling_function()\n",
    "# def llava_34b(image_name):\n",
    "#     root_path = '../prompting_framework/prompting_results/aircraft/'\n",
    "#     llava_7b_results = 'oxford-llava_34b.json'\n",
    "#     path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "#     with open(path_to_llava_7b_results, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "\n",
    "#     return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/aircraft/'\n",
    "    llava_7b_results = 'aircraft-llava-13b.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name][\"label\"] if data[image_name][\"label\"] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/aircraft/'\n",
    "    llava_7b_results = 'aircraft-bakllava.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name][\"label\"] if data[image_name][\"label\"] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/aircraft/'\n",
    "    llava_7b_results = 'aircraft-llava-llama3.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name][\"label\"] if data[image_name][\"label\"] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/aircraft/'\n",
    "    llava_7b_results = 'aircraft-moondream.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name][\"label\"] if data[image_name][\"label\"] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/aircraft/'\n",
    "    llava_7b_results = 'aircraft-llava-phi3.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name][\"label\"] if data[image_name][\"label\"] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def llama3_2_vision(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/aircraft/'\n",
    "    llava_7b_results = 'aircraft-llama3.2-vision-11b.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name][\"label\"] if data[image_name][\"label\"] is not None else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32a1dc4-80b9-4f37-83a0-4e1ecfddcadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2367 images in the Train set.\n",
      "There are 2365 images in the val set.\n",
      "There are 2368 images in the test set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../prompting_framework/prompting_results/aircraft/aircraft-llava-phi3-train-raw_info.json'\n",
    "val_data_json_path = '../prompting_framework/prompting_results/aircraft/aircraft-llava-phi3-val-raw_info.json'\n",
    "test_data_json_path = '../prompting_framework/prompting_results/aircraft/aircraft-llava-phi3-test-raw_info.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(val_data_json_path, 'r') as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "with open(test_data_json_path, 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for item in train_data:\n",
    "    train_image_names.append(item)\n",
    "\n",
    "val_image_names = []\n",
    "Y_val = []\n",
    "for item in val_data:\n",
    "    val_image_names.append(item)\n",
    "    Y_val.append(val_data[item][\"label\"])\n",
    "\n",
    "test_image_names = []\n",
    "Y_test = []\n",
    "for item in test_data:\n",
    "    test_image_names.append(item)\n",
    "    Y_test.append(test_data[item][\"label\"])\n",
    "\n",
    "# with open(dev_data_json_path, 'r') as file:\n",
    "#     dev_data = json.load(file)\n",
    "    \n",
    "# dev_image_names = []\n",
    "# Y_dev = []\n",
    "# for item in dev_data:\n",
    "#     Y_dev.append(dev_data[item])\n",
    "#     dev_image_names.append(item)\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(val_image_names)} images in the val set.\")\n",
    "print(f\"There are {len(test_image_names)} images in the test set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c8e9f68-b0fc-4af4-af82-764b95f6cb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_2_vision(train_image_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31df70bc-fe99-4432-8da3-2b68205fd06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "list_of_all_the_models = [\n",
    "        'llava_13b',\n",
    "       'llava_7b',\n",
    "       'llava_llama3',\n",
    "       'moondream',\n",
    "       'bakllava',\n",
    "       'llama3_2_vision',\n",
    "       'llava_phi3'\n",
    "       ]\n",
    "\n",
    "lfs = [llava_13b,\n",
    "       llava_7b,\n",
    "       llava_llama3,\n",
    "       moondream,\n",
    "       llama3_2_vision,\n",
    "       llava_phi3\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55a44531-b751-4c7c-8694-21da5af42af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2368it [00:53, 44.61it/s]\n",
      "2365it [00:52, 44.97it/s]\n",
      "2367it [00:52, 44.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_test = applier.apply(test_image_names)\n",
    "L_val = applier.apply(val_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bce83cdf-5a35-435f-90d6-a49e7bd8bcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977590</td>\n",
       "      <td>651</td>\n",
       "      <td>1714</td>\n",
       "      <td>0.275264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977590</td>\n",
       "      <td>438</td>\n",
       "      <td>1927</td>\n",
       "      <td>0.185201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 3, 4, 6, 7, 8, 9, 10, 13, 15, 17, 18, 2...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977590</td>\n",
       "      <td>661</td>\n",
       "      <td>1704</td>\n",
       "      <td>0.279493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moondream</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1, 4, 6, 8, 15, 16, 18, 22, 23, 24, 25, 26...</td>\n",
       "      <td>0.991121</td>\n",
       "      <td>0.991121</td>\n",
       "      <td>0.970402</td>\n",
       "      <td>269</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.114761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3_2_vision</th>\n",
       "      <td>4</td>\n",
       "      <td>[2, 4, 5, 8, 9, 10, 11, 12, 19, 20, 21, 26]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977590</td>\n",
       "      <td>865</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.365751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977590</td>\n",
       "      <td>686</td>\n",
       "      <td>1679</td>\n",
       "      <td>0.290063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 j                                           Polarity  \\\n",
       "llava_13b        0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...   \n",
       "llava_7b         1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...   \n",
       "llava_llama3     2  [0, 1, 3, 4, 6, 7, 8, 9, 10, 13, 15, 17, 18, 2...   \n",
       "moondream        3  [0, 1, 4, 6, 8, 15, 16, 18, 22, 23, 24, 25, 26...   \n",
       "llama3_2_vision  4        [2, 4, 5, 8, 9, 10, 11, 12, 19, 20, 21, 26]   \n",
       "llava_phi3       5  [0, 1, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, ...   \n",
       "\n",
       "                 Coverage  Overlaps  Conflicts  Correct  Incorrect  Emp. Acc.  \n",
       "llava_13b        1.000000  1.000000   0.977590      651       1714   0.275264  \n",
       "llava_7b         1.000000  1.000000   0.977590      438       1927   0.185201  \n",
       "llava_llama3     1.000000  1.000000   0.977590      661       1704   0.279493  \n",
       "moondream        0.991121  0.991121   0.970402      269       2075   0.114761  \n",
       "llama3_2_vision  1.000000  1.000000   0.977590      865       1500   0.365751  \n",
       "llava_phi3       1.000000  1.000000   0.977590      686       1679   0.290063  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val = np.array(Y_val)\n",
    "LFAnalysis(L_val, lfs).lf_summary(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12c18398-115a-4cbf-af86-4a02a0aa4d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 1...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984797</td>\n",
       "      <td>639</td>\n",
       "      <td>1729</td>\n",
       "      <td>0.269848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984797</td>\n",
       "      <td>429</td>\n",
       "      <td>1939</td>\n",
       "      <td>0.181166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, 15, 1...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984797</td>\n",
       "      <td>657</td>\n",
       "      <td>1711</td>\n",
       "      <td>0.277449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moondream</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1, 4, 6, 8, 13, 15, 16, 18, 19, 22, 23, 24...</td>\n",
       "      <td>0.994088</td>\n",
       "      <td>0.994088</td>\n",
       "      <td>0.979730</td>\n",
       "      <td>287</td>\n",
       "      <td>2067</td>\n",
       "      <td>0.121920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3_2_vision</th>\n",
       "      <td>4</td>\n",
       "      <td>[3, 4, 5, 8, 9, 10, 11, 12, 16, 20, 21, 26]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984797</td>\n",
       "      <td>869</td>\n",
       "      <td>1499</td>\n",
       "      <td>0.366976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984797</td>\n",
       "      <td>676</td>\n",
       "      <td>1692</td>\n",
       "      <td>0.285473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 j                                           Polarity  \\\n",
       "llava_13b        0  [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 1...   \n",
       "llava_7b         1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "llava_llama3     2  [0, 1, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, 15, 1...   \n",
       "moondream        3  [0, 1, 4, 6, 8, 13, 15, 16, 18, 19, 22, 23, 24...   \n",
       "llama3_2_vision  4        [3, 4, 5, 8, 9, 10, 11, 12, 16, 20, 21, 26]   \n",
       "llava_phi3       5  [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15...   \n",
       "\n",
       "                 Coverage  Overlaps  Conflicts  Correct  Incorrect  Emp. Acc.  \n",
       "llava_13b        1.000000  1.000000   0.984797      639       1729   0.269848  \n",
       "llava_7b         1.000000  1.000000   0.984797      429       1939   0.181166  \n",
       "llava_llama3     1.000000  1.000000   0.984797      657       1711   0.277449  \n",
       "moondream        0.994088  0.994088   0.979730      287       2067   0.121920  \n",
       "llama3_2_vision  1.000000  1.000000   0.984797      869       1499   0.366976  \n",
       "llava_phi3       1.000000  1.000000   0.984797      676       1692   0.285473  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = np.array(Y_test)\n",
    "LFAnalysis(L_test, lfs).lf_summary(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40937db9-7324-45fd-9072-70ad781703be",
   "metadata": {},
   "source": [
    "# Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64df0de9-63c3-4569-8d2b-f74336f6f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_exclude_negative(labels):\n",
    "    result = np.empty(labels.shape[0], dtype=labels.dtype)\n",
    "    for i, row in enumerate(labels):\n",
    "        # Get the unique values and their counts\n",
    "        values, counts = np.unique(row, return_counts=True)\n",
    "        # Sort both arrays by counts in descending order\n",
    "        sorted_indices = np.argsort(-counts)\n",
    "        values_sorted = values[sorted_indices]\n",
    "        counts_sorted = counts[sorted_indices]\n",
    "        \n",
    "        # Exclude -1 from the majority vote\n",
    "        if values_sorted[0] == -1:\n",
    "            if len(values_sorted) > 1:\n",
    "                result[i] = values_sorted[1]  # Use the second most frequent if -1 is the most frequent\n",
    "            else:\n",
    "                result[i] = -1  # If -1 is the only class, we have no choice but to use it\n",
    "        else:\n",
    "            result[i] = values_sorted[0]  # Most frequent non-negative value\n",
    "\n",
    "    return result\n",
    "    \n",
    "def majority_vote(labels):\n",
    "    # Assuming the labels are categorical and using mode to find the most frequent label\n",
    "    from scipy.stats import mode\n",
    "    # Using mode along axis=1 to find the most common element across columns\n",
    "    modes = mode(labels, axis=1)\n",
    "    # modes.mode contains the most common values, reshaping to (500,) for a clean 1D array output\n",
    "    return modes.mode.reshape(-1)\n",
    "\n",
    "# Applying the majority vote function\n",
    "majority_labels_val = majority_vote(L_val)\n",
    "majority_labels_exclude_negative_val = majority_vote_exclude_negative(L_val)\n",
    "\n",
    "majority_labels_test = majority_vote(L_test)\n",
    "majority_labels_exclude_negative_test = majority_vote_exclude_negative(L_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24e84f84-62a4-4d27-8a9a-6787c2f29d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.038737883051907444\n",
      "Recall: 0.04128589562544011\n",
      "F1 Score: 0.029212981612122126\n",
      "Accuracy: 0.3014799154334038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(Y_val, majority_labels_exclude_negative_val)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4852f187-098b-4506-8893-3372365b5d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.062406571539506885\n",
      "Recall: 0.04929418057444379\n",
      "F1 Score: 0.039311900141874065\n",
      "Accuracy: 0.30532094594594594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(Y_test, majority_labels_exclude_negative_test)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43606aa2-a574-4114-9ede-fd9b3528e43f",
   "metadata": {},
   "source": [
    "# Snorkel Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b347924c-bb09-4b06-a7b9-9089dc703872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 836.20epoch/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=30, verbose=False)\n",
    "label_model.fit(L_train, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6e79242-3f22-46b5-876c-d3b0a9aa940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "Precision: 0.10078015425144125\n",
      "Recall: 0.054599945384629894\n",
      "F1 Score: 0.05644668583644148\n",
      "Accuracy: 0.21733615221987315\n",
      "Test:\n",
      "Precision: 0.059306018253837016\n",
      "Recall: 0.05732430296328835\n",
      "F1 Score: 0.05509352861126802\n",
      "Accuracy: 0.20777027027027026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_val = label_model.predict_proba(L_val)\n",
    "preds_val = probs_to_preds(probs_val)\n",
    "\n",
    "probs_test = label_model.predict_proba(L_test)\n",
    "preds_test = probs_to_preds(probs_test)\n",
    "\n",
    "print(\"Validation:\")\n",
    "metrics = calculate_metrics(Y_val, preds_val)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "metrics = calculate_metrics(Y_test, preds_test)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8454a03b-6538-4141-90fa-bd69dfe19ddd",
   "metadata": {},
   "source": [
    "# Hyper Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d07254d-742f-47b2-b3a5-d4785554bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/hyperlm/hyper_label_model.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device(self.device))\n"
     ]
    }
   ],
   "source": [
    "from hyperlm import HyperLabelModel\n",
    "hlm = HyperLabelModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "600eb983-451a-43e4-9a52-61e8f133b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "Precision: 0.13212226112175626\n",
      "Recall: 0.07916468727442784\n",
      "F1 Score: 0.08939258076412132\n",
      "Accuracy: 0.27568710359408033\n",
      "Test:\n",
      "Precision: 0.12510123602945414\n",
      "Recall: 0.06955357425663299\n",
      "F1 Score: 0.07978793487838003\n",
      "Accuracy: 0.2690033783783784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "hyper_pred_val = hlm.infer(L_val[:,:])\n",
    "hyper_pred_test = hlm.infer(L_test[:,:])\n",
    "hyper_pred_train = hlm.infer(L_train)\n",
    "\n",
    "print(\"Validation:\")\n",
    "metrics = calculate_metrics(Y_val, hyper_pred_val)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "metrics = calculate_metrics(Y_test, hyper_pred_test)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372cd0e9-f33a-4dc0-81c8-b0e190daea23",
   "metadata": {},
   "source": [
    "# Fine-tuned Hyper Label Model on The Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b2b48fa-7f96-4085-9872-f30a105dae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/hyperlm/hyper_label_model.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device(self.device))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/hyperlm/hyper_label_model.py:248: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "Precision: 0.05136417662039886\n",
      "Recall: 0.09770175527827062\n",
      "F1 Score: 0.06387304061867036\n",
      "Accuracy: 0.35095137420718814\n",
      "Test:\n",
      "Precision: 0.05076994330402457\n",
      "Recall: 0.09945761435787251\n",
      "F1 Score: 0.06405820829838553\n",
      "Accuracy: 0.3450168918918919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from hyperlm import HyperLabelModel\n",
    "hlm = HyperLabelModel()\n",
    "\n",
    "val_test_labels = np.concatenate((Y_val,Y_test))\n",
    "L_val_test_agg = np.concatenate((L_val,L_test),axis=0)\n",
    "\n",
    "val_indices = list(range(Y_val.shape[0]))\n",
    "\n",
    "hyper_pred_val_test = hlm.infer(L_val_test_agg, y_indices=val_indices, y_vals=Y_val)\n",
    "\n",
    "ft_hyper_pred_val = hyper_pred_val_test[:Y_val.shape[0]]\n",
    "ft_hyper_pred_test = hyper_pred_val_test[Y_val.shape[0]:]\n",
    "\n",
    "print(\"Validation:\")\n",
    "metrics = calculate_metrics(Y_val, ft_hyper_pred_val)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "metrics = calculate_metrics(Y_test, ft_hyper_pred_test)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49f25bb6-7c01-4fc1-a495-ba6d47fdfa20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4733,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbb86a04-cc4a-4b7a-be58-3dbabcbd183e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2365,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aacaed98-2404-4f4f-8edb-bb37302acf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2365,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_pred_val_test[:Y_val.shape[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d9dcf55-d097-4548-bc63-af8a2ff7cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/hyperlm/hyper_label_model.py:248: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ee4be81-2302-44a6-884d-40ad536abac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50b00687-8920-4615-82c3-1d3c435ca942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4733, 6)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794ae57-d864-460e-9e90-7a28c72e7ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
