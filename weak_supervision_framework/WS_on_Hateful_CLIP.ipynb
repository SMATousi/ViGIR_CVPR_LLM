{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8ba67b3-f20f-4fc7-bc0e-7fe989318046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from snorkel.labeling import LFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ab9369-9698-4e97-a1c7-4badfed6f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered)\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered)\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered)\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered)\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126d3a76-347c-4aac-9612-4e724b4e45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava:7b_results_hateful.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava 13b-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'bakllava-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-llama3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-phi3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'moondream-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_34b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava 34b-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a34cf9-008a-466d-b984-28efbd09b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8500 images in the Train set.\n",
      "There are 500 images in the dev set.\n",
      "There are 500 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_train.json'\n",
    "dev_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_dev.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for entry in train_data:\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    train_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for entry in dev_data:\n",
    "    Y_dev.append(entry['label'])\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    dev_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb723da-0112-44f4-8a98-41151bf12100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lfs = [moondream,\n",
    "       llava_34b,\n",
    "       llava_13b,\n",
    "       llava_phi3,\n",
    "       bakllava,\n",
    "       llava_7b,\n",
    "       llava_llama3\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5a98a4-7e96-4b11-8496-d98f9557b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:03, 140.22it/s]\n",
      "8500it [01:02, 136.42it/s]\n"
     ]
    }
   ],
   "source": [
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf1e087-8ed2-4441-84eb-805a09899b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>moondream</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>246</td>\n",
       "      <td>254</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_34b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>269</td>\n",
       "      <td>231</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>288</td>\n",
       "      <td>212</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.744</td>\n",
       "      <td>267</td>\n",
       "      <td>228</td>\n",
       "      <td>0.539394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.710</td>\n",
       "      <td>276</td>\n",
       "      <td>199</td>\n",
       "      <td>0.581053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>298</td>\n",
       "      <td>202</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>278</td>\n",
       "      <td>222</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "moondream     0   [0, 1]      1.00      1.00      0.752      246        254   \n",
       "llava_34b     1   [0, 1]      1.00      1.00      0.752      269        231   \n",
       "llava_13b     2   [0, 1]      1.00      1.00      0.752      288        212   \n",
       "llava_phi3    3   [0, 1]      0.99      0.99      0.744      267        228   \n",
       "bakllava      4   [0, 1]      0.95      0.95      0.710      276        199   \n",
       "llava_7b      5   [0, 1]      1.00      1.00      0.752      298        202   \n",
       "llava_llama3  6   [0, 1]      1.00      1.00      0.752      278        222   \n",
       "\n",
       "              Emp. Acc.  \n",
       "moondream      0.492000  \n",
       "llava_34b      0.538000  \n",
       "llava_13b      0.576000  \n",
       "llava_phi3     0.539394  \n",
       "bakllava       0.581053  \n",
       "llava_7b       0.596000  \n",
       "llava_llama3   0.556000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12099889-2763-4aec-b73f-964197454bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3585.12epoch/s]\n"
     ]
    }
   ],
   "source": [
    "label_model = LabelModel(cardinality=2, verbose=False)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe0ed5-5dcb-40ee-9d1e-55ffb59b29fe",
   "metadata": {},
   "source": [
    "# Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa1c3b7c-713c-4885-bd0c-bd39b8689ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 12, 123, 1234, 12345]\n",
    "epochs = [100, 200, 300, 400, 800, 1000, 2000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38fa7786-5e25-44e1-bdf2-361bb29af8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 3993.13epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4109.89epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4211.56epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4172.43epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4192.22epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4313.65epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4255.11epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4280.17epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4129.19epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4175.29epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4235.14epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4197.10epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4214.88epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4328.57epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4333.43epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4367.85epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4143.30epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4219.20epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4313.86epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4318.07epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4396.97epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4456.63epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4388.37epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4438.64epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 3677.09epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4065.15epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4225.59epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4307.68epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4376.75epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4396.36epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4424.77epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4433.52epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4158.42epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4129.74epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4343.56epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4392.13epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4390.79epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4388.10epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4425.12epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4453.52epoch/s]\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    for epoch in epochs:\n",
    "        label_model = LabelModel(cardinality=2, verbose=False)\n",
    "        label_model.fit(L_train, Y_dev, n_epochs=epoch, log_freq=500, seed=seed)\n",
    "\n",
    "        probs_dev = label_model.predict_proba(L_dev)\n",
    "        preds_dev = probs_to_preds(probs_dev)\n",
    "        \n",
    "        metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "\n",
    "        all_results.append([metrics['F1 Score'], epoch, seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6345cd6-260c-428f-b713-fc901568fe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5529157667386609, 100, 1],\n",
       " [0.5529157667386609, 200, 1],\n",
       " [0.548936170212766, 300, 1],\n",
       " [0.548936170212766, 400, 1],\n",
       " [0.548936170212766, 800, 1],\n",
       " [0.548936170212766, 1000, 1],\n",
       " [0.548936170212766, 2000, 1],\n",
       " [0.548936170212766, 5000, 1],\n",
       " [0.5529157667386609, 100, 12],\n",
       " [0.548936170212766, 200, 12],\n",
       " [0.548936170212766, 300, 12],\n",
       " [0.548936170212766, 400, 12],\n",
       " [0.548936170212766, 800, 12],\n",
       " [0.548936170212766, 1000, 12],\n",
       " [0.548936170212766, 2000, 12],\n",
       " [0.548936170212766, 5000, 12],\n",
       " [0.5529157667386609, 100, 123],\n",
       " [0.5529157667386609, 200, 123],\n",
       " [0.548936170212766, 300, 123],\n",
       " [0.548936170212766, 400, 123],\n",
       " [0.548936170212766, 800, 123],\n",
       " [0.548936170212766, 1000, 123],\n",
       " [0.548936170212766, 2000, 123],\n",
       " [0.548936170212766, 5000, 123],\n",
       " [0.5529157667386609, 100, 1234],\n",
       " [0.548936170212766, 200, 1234],\n",
       " [0.548936170212766, 300, 1234],\n",
       " [0.548936170212766, 400, 1234],\n",
       " [0.548936170212766, 800, 1234],\n",
       " [0.548936170212766, 1000, 1234],\n",
       " [0.548936170212766, 2000, 1234],\n",
       " [0.548936170212766, 5000, 1234],\n",
       " [0.5529157667386609, 100, 12345],\n",
       " [0.5529157667386609, 200, 12345],\n",
       " [0.548936170212766, 300, 12345],\n",
       " [0.548936170212766, 400, 12345],\n",
       " [0.548936170212766, 800, 12345],\n",
       " [0.548936170212766, 1000, 12345],\n",
       " [0.548936170212766, 2000, 12345],\n",
       " [0.548936170212766, 5000, 12345]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093a7a9-f985-485e-a4a3-e3d98381dc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb3f6e-1c44-4fb4-a17a-0e49e7419dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e758d7a3-2aeb-47e5-abaf-07276e6ddf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 3917.02epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[165  85]\n",
      " [122 128]]\n",
      "Precision: 0.6009389671361502\n",
      "Recall: 0.512\n",
      "F1 Score: 0.5529157667386609\n",
      "Accuracy: 0.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=False)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=100, log_freq=500, seed=12)\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f1f23-acc2-4314-a2be-4028b8f07e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f333a54-c4ed-4223-9de5-f20ec98bb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def expected_cross_entropy_loss(logits, target_distributions):\n",
    "    \"\"\"\n",
    "    Computes the expected cross-entropy loss for a batch of predictions and target distributions.\n",
    "\n",
    "    Parameters:\n",
    "    logits (torch.Tensor): The raw output from the model of shape (batch_size, num_classes).\n",
    "    target_distributions (torch.Tensor): The target class distributions of shape (batch_size, num_classes),\n",
    "                                         where each row is a probability distribution over classes.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The expected cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    \n",
    "    # Compute the element-wise product between target distributions and log probabilities\n",
    "    # Then, sum across classes to get the cross-entropy for each instance\n",
    "    cross_entropy = -torch.sum(target_distributions * log_probs, dim=1)\n",
    "    \n",
    "    # Take the mean over the batch\n",
    "    loss = cross_entropy.mean()\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, image_names, root_dir, labels, target_dists, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_frame (DataFrame): DataFrame containing image names and labels.\n",
    "            image_dir (str): Directory where the images are stored.\n",
    "            processor (CLIPProcessor): CLIP processor for preprocessing images.\n",
    "        \"\"\"\n",
    "        self.image_names = image_names\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.target_dists = target_dists\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image name and label from the dataframe\n",
    "        img_name = os.path.join(self.root_dir, self.image_names[idx])\n",
    "        label = self.labels[idx]\n",
    "        target_dist = self.target_dists[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        # Return image and label\n",
    "        return inputs['pixel_values'].squeeze(0), torch.tensor(label, dtype=torch.long), torch.tensor(target_dist)\n",
    "\n",
    "# MLP head to be added after the CLIP model\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# CLIP model with MLP head for binary classification\n",
    "class CLIPWithMLP(nn.Module):\n",
    "    def __init__(self, clip_model, mlp_head):\n",
    "        super(CLIPWithMLP, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.mlp_head = mlp_head\n",
    "\n",
    "        # Freeze CLIP's parameters\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Extract image features from CLIP\n",
    "        image_features = self.clip_model.get_image_features(pixel_values=image)\n",
    "        # Pass through the MLP head\n",
    "        outputs = self.mlp_head(image_features)\n",
    "        return outputs\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, dev_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels, target_dist in tqdm(train_loader):\n",
    "            images, labels, target_dist = images.to(device), labels.to(device), target_dist.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # loss = expected_cross_entropy_loss(outputs, target_dist)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        evaluate_model(model, dev_loader, device)\n",
    "\n",
    "# Evaluation function to compute precision, recall, and F1-score\n",
    "def evaluate_model(model, dev_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_dist in tqdm(dev_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = calculate_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "390c1764-11fc-4346-9978-c1dc68efdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:13<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.6845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:05<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 32 218]\n",
      " [ 19 231]]\n",
      "Precision: 0.5144766146993318\n",
      "Recall: 0.924\n",
      "F1 Score: 0.6609442060085837\n",
      "Accuracy: 0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:05<00:00, 11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 32 218]\n",
      " [ 19 231]]\n",
      "Precision: 0.5144766146993318\n",
      "Recall: 0.924\n",
      "F1 Score: 0.6609442060085837\n",
      "Accuracy: 0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Confusion Matrix': array([[ 32, 218],\n",
       "        [ 19, 231]]),\n",
       " 'Precision': 0.5144766146993318,\n",
       " 'Recall': 0.924,\n",
       " 'F1 Score': 0.6609442060085837,\n",
       " 'Accuracy': 0.526}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "root_dir = \"/home1/pupil/goowfd/CVPR_2025/hateful_memes/img/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "probs_train = label_model.predict_proba(L_train)\n",
    "preds_train = probs_to_preds(probs_train)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = HatefulMemesDataset(image_names=train_image_names, \n",
    "                                    root_dir=root_dir, \n",
    "                                    labels=preds_train,\n",
    "                                    target_dists=probs_train,\n",
    "                                    processor=processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=16)\n",
    "\n",
    "dev_dataset = HatefulMemesDataset(image_names=dev_image_names, \n",
    "                                  root_dir=root_dir, \n",
    "                                  labels=Y_dev, \n",
    "                                  target_dists=probs_dev,\n",
    "                                  processor=processor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Define MLP head (the dimension is based on CLIP output size)\n",
    "mlp_head = MLPHead(input_dim=512, output_dim=2)  # Binary classification, so output_dim = 2\n",
    "\n",
    "# Create the full model with CLIP + MLP\n",
    "model = CLIPWithMLP(clip_model=clip_model, mlp_head=mlp_head)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.mlp_head.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 1\n",
    "train_model(model, train_loader, dev_loader, criterion, optimizer, device, epochs=epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, dev_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f67f449-fdbd-404b-84c3-301ba3f93cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:05<00:00, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 32 218]\n",
      " [ 19 231]]\n",
      "Precision: 0.5144766146993318\n",
      "Recall: 0.924\n",
      "F1 Score: 0.6609442060085837\n",
      "Accuracy: 0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Confusion Matrix': array([[ 32, 218],\n",
       "        [ 19, 231]]),\n",
       " 'Precision': 0.5144766146993318,\n",
       " 'Recall': 0.924,\n",
       " 'F1 Score': 0.6609442060085837,\n",
       " 'Accuracy': 0.526}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, dev_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a88a2-4f28-41ea-b8b3-57cf6564dab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bee2e6-e8a7-4c84-b425-6ba34b0d0ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
