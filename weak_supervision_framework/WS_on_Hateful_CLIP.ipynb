{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ba67b3-f20f-4fc7-bc0e-7fe989318046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from snorkel.labeling import LFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ab9369-9698-4e97-a1c7-4badfed6f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered)\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered)\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered)\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered)\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126d3a76-347c-4aac-9612-4e724b4e45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava:7b_results_hateful.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava 13b-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'bakllava-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-llama3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-phi3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'moondream-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_34b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava 34b-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a34cf9-008a-466d-b984-28efbd09b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8500 images in the Train set.\n",
      "There are 500 images in the dev set.\n",
      "There are 500 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_train.json'\n",
    "dev_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_dev.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for entry in train_data:\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    train_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for entry in dev_data:\n",
    "    Y_dev.append(entry['label'])\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    dev_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eb723da-0112-44f4-8a98-41151bf12100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lfs = [moondream,\n",
    "       llava_34b,\n",
    "       llava_13b,\n",
    "       llava_phi3,\n",
    "       bakllava,\n",
    "       llava_7b,\n",
    "       llava_llama3\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e5a98a4-7e96-4b11-8496-d98f9557b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:07, 69.58it/s]\n",
      "8500it [01:10, 121.43it/s]\n"
     ]
    }
   ],
   "source": [
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf1e087-8ed2-4441-84eb-805a09899b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>moondream</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>246</td>\n",
       "      <td>254</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_34b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>269</td>\n",
       "      <td>231</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>288</td>\n",
       "      <td>212</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.744</td>\n",
       "      <td>267</td>\n",
       "      <td>228</td>\n",
       "      <td>0.539394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.710</td>\n",
       "      <td>276</td>\n",
       "      <td>199</td>\n",
       "      <td>0.581053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>298</td>\n",
       "      <td>202</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.752</td>\n",
       "      <td>278</td>\n",
       "      <td>222</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "moondream     0   [0, 1]      1.00      1.00      0.752      246        254   \n",
       "llava_34b     1   [0, 1]      1.00      1.00      0.752      269        231   \n",
       "llava_13b     2   [0, 1]      1.00      1.00      0.752      288        212   \n",
       "llava_phi3    3   [0, 1]      0.99      0.99      0.744      267        228   \n",
       "bakllava      4   [0, 1]      0.95      0.95      0.710      276        199   \n",
       "llava_7b      5   [0, 1]      1.00      1.00      0.752      298        202   \n",
       "llava_llama3  6   [0, 1]      1.00      1.00      0.752      278        222   \n",
       "\n",
       "              Emp. Acc.  \n",
       "moondream      0.492000  \n",
       "llava_34b      0.538000  \n",
       "llava_13b      0.576000  \n",
       "llava_phi3     0.539394  \n",
       "bakllava       0.581053  \n",
       "llava_7b       0.596000  \n",
       "llava_llama3   0.556000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12099889-2763-4aec-b73f-964197454bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4048.92epoch/s]\n"
     ]
    }
   ],
   "source": [
    "label_model = LabelModel(cardinality=2, verbose=False)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe0ed5-5dcb-40ee-9d1e-55ffb59b29fe",
   "metadata": {},
   "source": [
    "# Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa1c3b7c-713c-4885-bd0c-bd39b8689ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 12, 123, 1234, 12345]\n",
    "epochs = [100, 200, 300, 400, 800, 1000, 2000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38fa7786-5e25-44e1-bdf2-361bb29af8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4202.08epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4303.99epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4204.24epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4364.38epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4398.93epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4421.53epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4460.32epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4413.82epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4304.01epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4385.76epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4504.48epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4490.69epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4491.48epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4517.71epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4503.80epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4464.63epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4296.34epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4350.51epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4356.63epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4458.30epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4447.55epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4382.56epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4399.65epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4364.42epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4248.99epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4294.89epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4347.10epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4377.78epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4341.59epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4366.62epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4383.19epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4468.66epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4298.24epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4310.52epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4413.60epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4411.49epoch/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 4337.25epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4459.49epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4497.27epoch/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4551.41epoch/s]\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    for epoch in epochs:\n",
    "        label_model = LabelModel(cardinality=2, verbose=False)\n",
    "        label_model.fit(L_train, Y_dev, n_epochs=epoch, log_freq=500, seed=seed)\n",
    "\n",
    "        probs_dev = label_model.predict_proba(L_dev)\n",
    "        preds_dev = probs_to_preds(probs_dev)\n",
    "        \n",
    "        metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "\n",
    "        all_results.append([metrics['F1 Score'], epoch, seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6345cd6-260c-428f-b713-fc901568fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093a7a9-f985-485e-a4a3-e3d98381dc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb3f6e-1c44-4fb4-a17a-0e49e7419dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e758d7a3-2aeb-47e5-abaf-07276e6ddf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4219.96epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[165  85]\n",
      " [122 128]]\n",
      "Precision: 0.6009389671361502\n",
      "Recall: 0.512\n",
      "F1 Score: 0.5529157667386609\n",
      "Accuracy: 0.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=False)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=100, log_freq=500, seed=12)\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f1f23-acc2-4314-a2be-4028b8f07e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f333a54-c4ed-4223-9de5-f20ec98bb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def expected_cross_entropy_loss(logits, target_distributions):\n",
    "    \"\"\"\n",
    "    Computes the expected cross-entropy loss for a batch of predictions and target distributions.\n",
    "\n",
    "    Parameters:\n",
    "    logits (torch.Tensor): The raw output from the model of shape (batch_size, num_classes).\n",
    "    target_distributions (torch.Tensor): The target class distributions of shape (batch_size, num_classes),\n",
    "                                         where each row is a probability distribution over classes.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The expected cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    \n",
    "    # Compute the element-wise product between target distributions and log probabilities\n",
    "    # Then, sum across classes to get the cross-entropy for each instance\n",
    "    cross_entropy = -torch.sum(target_distributions * log_probs, dim=1)\n",
    "    \n",
    "    # Take the mean over the batch\n",
    "    loss = cross_entropy.mean()\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, image_names, root_dir, labels, target_dists, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_frame (DataFrame): DataFrame containing image names and labels.\n",
    "            image_dir (str): Directory where the images are stored.\n",
    "            processor (CLIPProcessor): CLIP processor for preprocessing images.\n",
    "        \"\"\"\n",
    "        self.image_names = image_names\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.target_dists = target_dists\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image name and label from the dataframe\n",
    "        img_name = os.path.join(self.root_dir, self.image_names[idx])\n",
    "        label = self.labels[idx]\n",
    "        target_dist = self.target_dists[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        # Return image and label\n",
    "        return inputs['pixel_values'].squeeze(0), torch.tensor(label, dtype=torch.long), torch.tensor(target_dist)\n",
    "\n",
    "# MLP head to be added after the CLIP model\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# CLIP model with MLP head for binary classification\n",
    "class CLIPWithMLP(nn.Module):\n",
    "    def __init__(self, clip_model, mlp_head):\n",
    "        super(CLIPWithMLP, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.mlp_head = mlp_head\n",
    "\n",
    "        # Freeze CLIP's parameters\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Extract image features from CLIP\n",
    "        image_features = self.clip_model.get_image_features(pixel_values=image)\n",
    "        # Pass through the MLP head\n",
    "        outputs = self.mlp_head(image_features)\n",
    "        return outputs\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, dev_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels, target_dist in tqdm(train_loader):\n",
    "            images, labels, target_dist = images.to(device), labels.to(device), target_dist.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # loss = expected_cross_entropy_loss(outputs, target_dist)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        evaluate_model(model, dev_loader, device)\n",
    "\n",
    "# Evaluation function to compute precision, recall, and F1-score\n",
    "def evaluate_model(model, dev_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_dist in tqdm(dev_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = calculate_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "390c1764-11fc-4346-9978-c1dc68efdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [01:53<00:00,  6.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.6774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:11<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 84 166]\n",
      " [ 52 198]]\n",
      "Precision: 0.5439560439560439\n",
      "Recall: 0.792\n",
      "F1 Score: 0.6449511400651465\n",
      "Accuracy: 0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:10<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 84 166]\n",
      " [ 52 198]]\n",
      "Precision: 0.5439560439560439\n",
      "Recall: 0.792\n",
      "F1 Score: 0.6449511400651465\n",
      "Accuracy: 0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Confusion Matrix': array([[ 84, 166],\n",
       "        [ 52, 198]]),\n",
       " 'Precision': 0.5439560439560439,\n",
       " 'Recall': 0.792,\n",
       " 'F1 Score': 0.6449511400651465,\n",
       " 'Accuracy': 0.564}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "root_dir = \"/home1/pupil/goowfd/CVPR_2025/hateful_memes/img/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "\n",
    "probs_train = label_model.predict_proba(L_train)\n",
    "preds_train = probs_to_preds(probs_train)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = HatefulMemesDataset(image_names=train_image_names, \n",
    "                                    root_dir=root_dir, \n",
    "                                    labels=preds_train,\n",
    "                                    target_dists=probs_train,\n",
    "                                    processor=processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=16)\n",
    "\n",
    "dev_dataset = HatefulMemesDataset(image_names=dev_image_names, \n",
    "                                  root_dir=root_dir, \n",
    "                                  labels=Y_dev, \n",
    "                                  target_dists=probs_dev,\n",
    "                                  processor=processor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Define MLP head (the dimension is based on CLIP output size)\n",
    "# mlp_head = MLPHead(input_dim=512, output_dim=2)  # Binary classification, so output_dim = 2\n",
    "mlp_head = MLPHead(input_dim=768, output_dim=2)  # Binary classification, so output_dim = 2\n",
    "\n",
    "# Create the full model with CLIP + MLP\n",
    "model = CLIPWithMLP(clip_model=clip_model, mlp_head=mlp_head)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.mlp_head.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 1\n",
    "train_model(model, train_loader, dev_loader, criterion, optimizer, device, epochs=epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, dev_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67f449-fdbd-404b-84c3-301ba3f93cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, dev_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a88a2-4f28-41ea-b8b3-57cf6564dab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bee2e6-e8a7-4c84-b425-6ba34b0d0ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
