{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dbf026f-8d8c-4a44-b8cc-0d5208f4ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8020ba11-64f2-49de-b04a-56c7d16da07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/eurosat/interpreter/'\n",
    "    llava_7b_results = 'eurosat_llava7b.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/eurosat/interpreter'\n",
    "    llava_7b_results = 'eurosat_llava13b.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/eurosat/interpreter'\n",
    "    llava_7b_results = 'eurosat_bakllava.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/eurosat/interpreter'\n",
    "    llava_7b_results = 'eurosat_llava_llama3.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/eurosat/interpreter'\n",
    "    llava_7b_results = 'eurosat_llava_phi3.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/eurosat/interpreter'\n",
    "    llava_7b_results = 'eurosat_moondream.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def llava_34b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/eurosat/interpreter'\n",
    "    llava_7b_results = 'eurosat_llava34b.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def minicpm(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/eurosat/interpreter'\n",
    "    llava_7b_results = 'eurosat_minicpm.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1772f5f3-ee27-419b-8166-84193c2dab32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13500 images in the Train set.\n",
      "There are 8100 images in the dev set.\n",
      "There are 8100 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../prompting_framework/prompting_results/eurosat/interpreter/train_gt.json'\n",
    "dev_data_json_path = '../prompting_framework/prompting_results/eurosat/interpreter/test_gt.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for item in train_data:\n",
    "    train_image_names.append(item)\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for item in dev_data:\n",
    "    Y_dev.append(dev_data[item])\n",
    "    dev_image_names.append(item)\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eedf586-d080-49bf-9a1c-add7d319ab42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bakllava(train_image_names[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f282ea1-9844-4098-a5ca-e198c19c1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "list_of_all_the_models = ['llava_34b',\n",
    "       'llava_13b',\n",
    "       'llava_phi3',\n",
    "       'llava_7b',\n",
    "       'llava_llama3',\n",
    "       'minicpm',\n",
    "       'bakllava'\n",
    "       ]\n",
    "\n",
    "lfs = [llava_34b,\n",
    "       llava_13b,\n",
    "       llava_phi3,\n",
    "       llava_7b,\n",
    "       llava_llama3,\n",
    "       minicpm,\n",
    "       bakllava\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ae513d-a2ef-4141-95b2-878cb6309e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8100it [01:58, 68.22it/s]\n",
      "13500it [03:36, 62.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a8e827-5d9b-4b19-a12e-78d06c7344db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_34b</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>0.901481</td>\n",
       "      <td>0.901358</td>\n",
       "      <td>0.765679</td>\n",
       "      <td>4462</td>\n",
       "      <td>2840</td>\n",
       "      <td>0.611065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>0.915062</td>\n",
       "      <td>0.914938</td>\n",
       "      <td>0.765309</td>\n",
       "      <td>4360</td>\n",
       "      <td>3052</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>0.340988</td>\n",
       "      <td>0.337654</td>\n",
       "      <td>0.280247</td>\n",
       "      <td>1085</td>\n",
       "      <td>1677</td>\n",
       "      <td>0.392831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>0.973704</td>\n",
       "      <td>0.965679</td>\n",
       "      <td>0.814444</td>\n",
       "      <td>4367</td>\n",
       "      <td>3520</td>\n",
       "      <td>0.553696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>0.908025</td>\n",
       "      <td>0.905185</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>3016</td>\n",
       "      <td>4339</td>\n",
       "      <td>0.410061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicpm</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.768272</td>\n",
       "      <td>0.645802</td>\n",
       "      <td>2414</td>\n",
       "      <td>3841</td>\n",
       "      <td>0.385931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava</th>\n",
       "      <td>6</td>\n",
       "      <td>[1, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>0.187654</td>\n",
       "      <td>0.186914</td>\n",
       "      <td>0.174568</td>\n",
       "      <td>935</td>\n",
       "      <td>585</td>\n",
       "      <td>0.615132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              j                        Polarity  Coverage  Overlaps  \\\n",
       "llava_34b     0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  0.901481  0.901358   \n",
       "llava_13b     1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  0.915062  0.914938   \n",
       "llava_phi3    2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  0.340988  0.337654   \n",
       "llava_7b      3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  0.973704  0.965679   \n",
       "llava_llama3  4        [1, 3, 4, 5, 6, 7, 8, 9]  0.908025  0.905185   \n",
       "minicpm       5  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  0.772222  0.768272   \n",
       "bakllava      6        [1, 3, 4, 5, 6, 7, 8, 9]  0.187654  0.186914   \n",
       "\n",
       "              Conflicts  Correct  Incorrect  Emp. Acc.  \n",
       "llava_34b      0.765679     4462       2840   0.611065  \n",
       "llava_13b      0.765309     4360       3052   0.588235  \n",
       "llava_phi3     0.280247     1085       1677   0.392831  \n",
       "llava_7b       0.814444     4367       3520   0.553696  \n",
       "llava_llama3   0.756667     3016       4339   0.410061  \n",
       "minicpm        0.645802     2414       3841   0.385931  \n",
       "bakllava       0.174568      935        585   0.615132  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fc01a19-55e8-4c54-9ae3-0aaf35c13bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f6988d4-c2f2-44a1-8640-4719ce95c8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llava_34b</td>\n",
       "      <td>0.636037</td>\n",
       "      <td>0.642638</td>\n",
       "      <td>0.611065</td>\n",
       "      <td>0.594943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llava_13b</td>\n",
       "      <td>0.574568</td>\n",
       "      <td>0.705909</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.556386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llava_phi3</td>\n",
       "      <td>0.322776</td>\n",
       "      <td>0.452704</td>\n",
       "      <td>0.392831</td>\n",
       "      <td>0.267953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llava_7b</td>\n",
       "      <td>0.562570</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>0.553696</td>\n",
       "      <td>0.508367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llava_llama3</td>\n",
       "      <td>0.410576</td>\n",
       "      <td>0.557944</td>\n",
       "      <td>0.410061</td>\n",
       "      <td>0.349638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>minicpm</td>\n",
       "      <td>0.395461</td>\n",
       "      <td>0.536678</td>\n",
       "      <td>0.385931</td>\n",
       "      <td>0.329697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bakllava</td>\n",
       "      <td>0.613103</td>\n",
       "      <td>0.486576</td>\n",
       "      <td>0.615132</td>\n",
       "      <td>0.510729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model    Recall  Precision  Accuracy  F1 Score\n",
       "0     llava_34b  0.636037   0.642638  0.611065  0.594943\n",
       "1     llava_13b  0.574568   0.705909  0.588235  0.556386\n",
       "2    llava_phi3  0.322776   0.452704  0.392831  0.267953\n",
       "3      llava_7b  0.562570   0.623810  0.553696  0.508367\n",
       "4  llava_llama3  0.410576   0.557944  0.410061  0.349638\n",
       "5       minicpm  0.395461   0.536678  0.385931  0.329697\n",
       "6      bakllava  0.613103   0.486576  0.615132  0.510729"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "# Example ground truth and predictions for six models\n",
    "# Replace these arrays with actual predictions from each model\n",
    "y_true = Y_dev\n",
    "predictions = {}\n",
    "\n",
    "for i in range(L_dev.shape[1]):\n",
    "    predictions[list_of_all_the_models[i]] = L_dev[:,i]\n",
    "    \n",
    "# Create a DataFrame to store confusion matrix results and metrics\n",
    "confusion_data = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    # Confusion Matrix\n",
    "    metrics = calculate_metrics(Y_dev, y_pred)\n",
    "\n",
    "    precision = metrics['Precision']\n",
    "    recall = metrics['Recall']\n",
    "    f1 = metrics['F1 Score']\n",
    "    accuracy = metrics['Accuracy']\n",
    "    # Append data\n",
    "    confusion_data.append([\n",
    "        model_name,\n",
    "        recall, precision, accuracy, f1\n",
    "    ])\n",
    "\n",
    "# Convert to a DataFrame for display\n",
    "confusion_df = pd.DataFrame(confusion_data, columns=[\n",
    "    'Model', \n",
    "    'Recall', 'Precision', 'Accuracy', 'F1 Score'\n",
    "])\n",
    "\n",
    "# Display the table with confusion matrix and metrics\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd74e98-336c-426c-b2ac-f8215eb4e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 2558.94epoch/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=10, verbose=False)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2959b2b8-c793-4db3-aed7-d3b9629cf645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6121511260118815\n",
      "Recall: 0.6347555555555555\n",
      "F1 Score: 0.6080876216368022\n",
      "Accuracy: 0.6227160493827161\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3c293-e4a1-4eea-acf8-738c3457a0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
