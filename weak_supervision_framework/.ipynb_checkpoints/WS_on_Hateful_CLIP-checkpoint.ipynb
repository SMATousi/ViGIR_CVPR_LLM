{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ba67b3-f20f-4fc7-bc0e-7fe989318046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from snorkel.labeling import LFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126d3a76-347c-4aac-9612-4e724b4e45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava:7b_results_hateful.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava 13b-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'bakllava-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-llama3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-phi3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'moondream-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a34cf9-008a-466d-b984-28efbd09b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8500 images in the Train set.\n",
      "There are 500 images in the dev set.\n",
      "There are 500 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/simplified_train.json'\n",
    "dev_data_json_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/simplified_dev.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for entry in train_data:\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    train_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for entry in dev_data:\n",
    "    Y_dev.append(entry['label'])\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    dev_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb723da-0112-44f4-8a98-41151bf12100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lfs = [llava_7b,\n",
    "       llava_13b,\n",
    "       moondream,\n",
    "       llava_llama3,\n",
    "       llava_phi3,\n",
    "       bakllava\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5a98a4-7e96-4b11-8496-d98f9557b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:03, 159.21it/s]\n",
      "8500it [00:55, 154.18it/s]\n"
     ]
    }
   ],
   "source": [
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf1e087-8ed2-4441-84eb-805a09899b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>298</td>\n",
       "      <td>202</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>288</td>\n",
       "      <td>212</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moondream</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>246</td>\n",
       "      <td>254</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>278</td>\n",
       "      <td>222</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.730</td>\n",
       "      <td>267</td>\n",
       "      <td>228</td>\n",
       "      <td>0.539394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.696</td>\n",
       "      <td>276</td>\n",
       "      <td>199</td>\n",
       "      <td>0.581053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "llava_7b      0   [0, 1]      1.00      1.00      0.738      298        202   \n",
       "llava_13b     1   [0, 1]      1.00      1.00      0.738      288        212   \n",
       "moondream     2   [0, 1]      1.00      1.00      0.738      246        254   \n",
       "llava_llama3  3   [0, 1]      1.00      1.00      0.738      278        222   \n",
       "llava_phi3    4   [0, 1]      0.99      0.99      0.730      267        228   \n",
       "bakllava      5   [0, 1]      0.95      0.95      0.696      276        199   \n",
       "\n",
       "              Emp. Acc.  \n",
       "llava_7b       0.596000  \n",
       "llava_13b      0.576000  \n",
       "moondream      0.492000  \n",
       "llava_llama3   0.556000  \n",
       "llava_phi3     0.539394  \n",
       "bakllava       0.581053  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12099889-2763-4aec-b73f-964197454bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|                                                                                                  | 0/5000 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=5.341]\n",
      "  3%|██▊                                                                                   | 160/5000 [00:00<00:03, 1597.24epoch/s]INFO:root:[500 epochs]: TRAIN:[loss=0.002]\n",
      " 18%|███████████████▌                                                                      | 905/5000 [00:00<00:01, 3247.30epoch/s]INFO:root:[1000 epochs]: TRAIN:[loss=0.002]\n",
      " 26%|█████████████████████▋                                                               | 1279/5000 [00:00<00:01, 3439.24epoch/s]INFO:root:[1500 epochs]: TRAIN:[loss=0.002]\n",
      " 33%|████████████████████████████                                                         | 1652/5000 [00:00<00:00, 3542.18epoch/s]INFO:root:[2000 epochs]: TRAIN:[loss=0.002]\n",
      " 48%|████████████████████████████████████████▋                                            | 2394/5000 [00:00<00:00, 3632.87epoch/s]INFO:root:[2500 epochs]: TRAIN:[loss=0.002]\n",
      " 55%|███████████████████████████████████████████████                                      | 2768/5000 [00:00<00:00, 3665.48epoch/s]INFO:root:[3000 epochs]: TRAIN:[loss=0.002]\n",
      " 63%|█████████████████████████████████████████████████████▍                               | 3140/5000 [00:00<00:00, 3681.66epoch/s]INFO:root:[3500 epochs]: TRAIN:[loss=0.002]\n",
      " 78%|██████████████████████████████████████████████████████████████████▏                  | 3893/5000 [00:01<00:00, 3728.32epoch/s]INFO:root:[4000 epochs]: TRAIN:[loss=0.002]\n",
      " 85%|████████████████████████████████████████████████████████████████████████▌            | 4267/5000 [00:01<00:00, 3731.38epoch/s]INFO:root:[4500 epochs]: TRAIN:[loss=0.002]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3583.71epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e758d7a3-2aeb-47e5-abaf-07276e6ddf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label model f1 score: 0.5539112050739958\n",
      "Label model roc-auc: 0.605344\n"
     ]
    }
   ],
   "source": [
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "print(\n",
    "    f\"Label model f1 score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='f1')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Label model roc-auc: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='roc_auc')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f1f23-acc2-4314-a2be-4028b8f07e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f333a54-c4ed-4223-9de5-f20ec98bb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, image_names, root_dir, labels, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_frame (DataFrame): DataFrame containing image names and labels.\n",
    "            image_dir (str): Directory where the images are stored.\n",
    "            processor (CLIPProcessor): CLIP processor for preprocessing images.\n",
    "        \"\"\"\n",
    "        self.image_names = image_names\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image name and label from the dataframe\n",
    "        img_name = os.path.join(self.root_dir, self.image_names[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        # Return image and label\n",
    "        return inputs['pixel_values'].squeeze(0), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# MLP head to be added after the CLIP model\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# CLIP model with MLP head for binary classification\n",
    "class CLIPWithMLP(nn.Module):\n",
    "    def __init__(self, clip_model, mlp_head):\n",
    "        super(CLIPWithMLP, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.mlp_head = mlp_head\n",
    "\n",
    "        # Freeze CLIP's parameters\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Extract image features from CLIP\n",
    "        image_features = self.clip_model.get_image_features(pixel_values=image)\n",
    "        # Pass through the MLP head\n",
    "        outputs = self.mlp_head(image_features)\n",
    "        return outputs\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function to compute precision, recall, and F1-score\n",
    "def evaluate_model(model, dev_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dev_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "390c1764-11fc-4346-9978-c1dc68efdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:30<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.5184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.4471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.4222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.4086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.3987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.3872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.3784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.3677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:32<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.3580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:10<00:00,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5850\n",
      "Recall: 0.5920\n",
      "F1 Score: 0.5885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5849802371541502, 0.592, 0.588469184890656)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"/home1/pupil/goowfd/CVPR_2025/hateful_memes/img/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "probs_train = label_model.predict_proba(L_train)\n",
    "preds_train = probs_to_preds(probs_train)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = HatefulMemesDataset(image_names=train_image_names, root_dir=root_dir, labels=preds_train, processor=processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=16)\n",
    "\n",
    "dev_dataset = HatefulMemesDataset(image_names=dev_image_names, root_dir=root_dir, labels=Y_dev, processor=processor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Define MLP head (the dimension is based on CLIP output size)\n",
    "mlp_head = MLPHead(input_dim=768, output_dim=2)  # Binary classification, so output_dim = 2\n",
    "\n",
    "# Create the full model with CLIP + MLP\n",
    "model = CLIPWithMLP(clip_model=clip_model, mlp_head=mlp_head)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.mlp_head.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, dev_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67f449-fdbd-404b-84c3-301ba3f93cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
