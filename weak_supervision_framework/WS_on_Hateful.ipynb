{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afef4df4-daff-47ec-82ea-15c732f6ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd70f09-fc21-423d-9784-47651ec6e3cd",
   "metadata": {},
   "source": [
    "# Defining the Labelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71386fb0-f98d-4e26-9e70-30b5b34cd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava:7b_results_hateful.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava 13b-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'bakllava-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-llama3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-phi3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'moondream-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a054ab41-d025-4800-97d0-a2661422f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(llava_7b(\"48132.png\"))\n",
    "print(llava_llama3(\"48132.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff94abf-a386-4f28-a46e-835e955e8c47",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3bc313-ce3f-4875-b92f-8ecd100992de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8500 images in the Train set.\n",
      "There are 500 images in the dev set.\n",
      "There are 500 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_train.json'\n",
    "dev_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_dev.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for entry in train_data:\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    train_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for entry in dev_data:\n",
    "    Y_dev.append(entry['label'])\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    dev_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5cecc-7c43-4b50-b2a8-6b31b253dc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5b5df0f-fdba-4f11-9d99-e7cfaf24c44f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Applying the LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7ca650-4742-43d6-bb6f-c853dd176269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "lfs = [llava_7b,\n",
    "       llava_13b,\n",
    "       moondream,\n",
    "       llava_llama3,\n",
    "       llava_phi3,\n",
    "       bakllava\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b475d082-aa6a-4fbf-8299-1b1237529a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:07, 65.27it/s]\n",
      "8500it [01:54, 74.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "757add37-1ef5-4fc8-937f-c7b963760125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>298</td>\n",
       "      <td>202</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>288</td>\n",
       "      <td>212</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moondream</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>246</td>\n",
       "      <td>254</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>278</td>\n",
       "      <td>222</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.730</td>\n",
       "      <td>267</td>\n",
       "      <td>228</td>\n",
       "      <td>0.539394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.696</td>\n",
       "      <td>276</td>\n",
       "      <td>199</td>\n",
       "      <td>0.581053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "llava_7b      0   [0, 1]      1.00      1.00      0.738      298        202   \n",
       "llava_13b     1   [0, 1]      1.00      1.00      0.738      288        212   \n",
       "moondream     2   [0, 1]      1.00      1.00      0.738      246        254   \n",
       "llava_llama3  3   [0, 1]      1.00      1.00      0.738      278        222   \n",
       "llava_phi3    4   [0, 1]      0.99      0.99      0.730      267        228   \n",
       "bakllava      5   [0, 1]      0.95      0.95      0.696      276        199   \n",
       "\n",
       "              Emp. Acc.  \n",
       "llava_7b       0.596000  \n",
       "llava_13b      0.576000  \n",
       "moondream      0.492000  \n",
       "llava_llama3   0.556000  \n",
       "llava_phi3     0.539394  \n",
       "bakllava       0.581053  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd038dc-0729-4e6f-9f14-805066f62eb6",
   "metadata": {},
   "source": [
    "F1 of the labelers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11214f33-2540-4854-a076-f581ca68b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered)\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered, average='weighted')\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered, average='weighted')\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered, average='weighted')\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d70b98e-2e01-4b26-8456-9d4d450668b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Confusion Matrix: [[144 106]\n",
      " [ 96 154]]\n",
      "Precision: 0.5961538461538461\n",
      "Recall: 0.596\n",
      "F1 Score: 0.5958383353341337\n",
      "Accuracy: 0.596\n",
      "-------------------------------\n",
      "Confusion Matrix: [[175  75]\n",
      " [137 113]]\n",
      "Precision: 0.5809806328423349\n",
      "Recall: 0.576\n",
      "F1 Score: 0.5693787679357806\n",
      "Accuracy: 0.576\n",
      "-------------------------------\n",
      "Confusion Matrix: [[238  12]\n",
      " [242   8]]\n",
      "Precision: 0.4479166666666667\n",
      "Recall: 0.492\n",
      "F1 Score: 0.3556570268899036\n",
      "Accuracy: 0.492\n",
      "-------------------------------\n",
      "Confusion Matrix: [[103 147]\n",
      " [ 75 175]]\n",
      "Precision: 0.5610649731314118\n",
      "Recall: 0.556\n",
      "F1 Score: 0.5465982615515326\n",
      "Accuracy: 0.556\n",
      "-------------------------------\n",
      "Confusion Matrix: [[143 107]\n",
      " [121 124]]\n",
      "Precision: 0.5392561983471075\n",
      "Recall: 0.5393939393939394\n",
      "F1 Score: 0.538893166038966\n",
      "Accuracy: 0.5393939393939394\n",
      "-------------------------------\n",
      "Confusion Matrix: [[164  72]\n",
      " [127 112]]\n",
      "Precision: 0.5862772574645938\n",
      "Recall: 0.5810526315789474\n",
      "F1 Score: 0.5756776804810957\n",
      "Accuracy: 0.5810526315789474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for i in range(L_dev.shape[1]):\n",
    "\n",
    "    print(\"-------------------------------\")\n",
    "    metrics = calculate_metrics(Y_dev, L_dev[:,i])\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec301f4-aa31-4004-bfc2-b85b2ba88de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d022c-4a6b-4258-86f8-71b615cfa8b3",
   "metadata": {},
   "source": [
    "# Optimization for Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84ea9e65-8d70-4621-b3af-f13500eaaf42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|                                                                                                                               | 0/5000 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=5.341]\n",
      "  7%|████████▌                                                                                                          | 373/5000 [00:00<00:03, 1253.11epoch/s]INFO:root:[500 epochs]: TRAIN:[loss=0.002]\n",
      " 18%|█████████████████████                                                                                              | 914/5000 [00:00<00:03, 1327.78epoch/s]INFO:root:[1000 epochs]: TRAIN:[loss=0.002]\n",
      " 29%|█████████████████████████████████▌                                                                                | 1470/5000 [00:01<00:02, 1376.93epoch/s]INFO:root:[1500 epochs]: TRAIN:[loss=0.002]\n",
      " 38%|███████████████████████████████████████████▏                                                                      | 1892/5000 [00:01<00:02, 1382.97epoch/s]INFO:root:[2000 epochs]: TRAIN:[loss=0.002]\n",
      " 49%|███████████████████████████████████████████████████████▉                                                          | 2453/5000 [00:01<00:01, 1363.23epoch/s]INFO:root:[2500 epochs]: TRAIN:[loss=0.002]\n",
      " 57%|█████████████████████████████████████████████████████████████████▌                                                | 2875/5000 [00:02<00:01, 1389.93epoch/s]INFO:root:[3000 epochs]: TRAIN:[loss=0.002]\n",
      " 68%|█████████████████████████████████████████████████████████████████████████████▉                                    | 3417/5000 [00:02<00:01, 1242.15epoch/s]INFO:root:[3500 epochs]: TRAIN:[loss=0.002]\n",
      " 79%|██████████████████████████████████████████████████████████████████████████████████████████▌                       | 3974/5000 [00:02<00:00, 1360.10epoch/s]INFO:root:[4000 epochs]: TRAIN:[loss=0.002]\n",
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████▎             | 4397/5000 [00:03<00:00, 1390.03epoch/s]INFO:root:[4500 epochs]: TRAIN:[loss=0.002]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:03<00:00, 1341.36epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427bf3f3-3d55-40e0-938a-a9a672fb88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[158  92]\n",
      " [119 131]]\n",
      "Precision: 0.5789205290508491\n",
      "Recall: 0.578\n",
      "F1 Score: 0.5767658492163148\n",
      "Accuracy: 0.578\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeee78a-8ef5-463e-b837-01e5e2399848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c11aac3-a236-48ed-b80f-f97875351d15",
   "metadata": {},
   "source": [
    "# Training the End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85550ff2-272c-4f20-acba-1cddf2ad952b",
   "metadata": {},
   "source": [
    "The dataloader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90e13508-d0fb-4f2c-89da-98c7961f60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom dataset class for loading images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_names, root_dir, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_names (list): List of image file names.\n",
    "            root_dir (string): Directory where images are stored.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_names = image_names\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Build the full path of the image file\n",
    "        img_name = os.path.join(self.root_dir, self.image_names[idx])\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_name).convert('RGB')  # Load image as RGB\n",
    "\n",
    "        # Apply any transformations (e.g., resize, normalization)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f892c82d-e848-41b8-9c58-98462f84401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home1/pupil/goowfd/CVPR_2025/hateful_memes/img/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224 (example)\n",
    "        transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean/std\n",
    "    ])\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_train)\n",
    "label_model_predictions = probs_to_preds(probs_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03c0e90b-c7be-44cf-bfe3-01a361aa2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(image_names=train_image_names, root_dir=root_dir, labels=label_model_predictions, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf71dc2-729a-4ba4-b8f4-33e438edc863",
   "metadata": {},
   "source": [
    "A basic ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f13e49-02cb-4a0d-9e38-ebe167e96c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Pretrained ResNet with MLP head for binary classification\n",
    "class ResNetWithMLP(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNetWithMLP, self).__init__()\n",
    "        # Load a pretrained ResNet (e.g., ResNet18)\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Freeze the ResNet layers if you don't want to train them\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the last fully connected layer with a custom MLP head\n",
    "        num_features = self.resnet.fc.in_features  # Get the number of features in the last layer\n",
    "        self.resnet.fc = MLPHead(input_dim=num_features, output_dim=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17dfac98-4198-4130-9117-060ff4c51175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNetWithMLP(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bcf001b-a515-4e9f-8d4e-2bf5114600b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85337a25-23c3-4bdb-98c6-3f921342c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:27<00:00, 38.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6554, Accuracy: 61.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:17<00:00, 60.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.6356, Accuracy: 64.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 62.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.6321, Accuracy: 64.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.6299, Accuracy: 64.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.6236, Accuracy: 65.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.6240, Accuracy: 64.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.6198, Accuracy: 65.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.6191, Accuracy: 65.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.6137, Accuracy: 65.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.6127, Accuracy: 66.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Binary classification loss\n",
    "optimizer = optim.Adam(model.resnet.fc.parameters(), lr=0.001)  # Only optimize the MLP parameters\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "train(model, dataloader, criterion, optimizer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea2d27-e855-481d-9df1-490cf74feb7d",
   "metadata": {},
   "source": [
    "Evaluation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd333283-c836-4eac-b7f3-44892acf6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate the model on the dev set\n",
    "def evaluate(model, dev_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels in tqdm(dev_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get predictions (class with the highest score)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Store true labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(labels, predictions):\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7526a040-0b04-4da3-8289-ffb05d17a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = ImageDataset(image_names=dev_image_names, root_dir=root_dir, labels=Y_dev, transform=transform)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d0443e3-46ec-44ac-af0c-06375b1924b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:01<00:00, 31.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5491\n",
      "Recall: 0.4920\n",
      "F1 Score: 0.5190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5491071428571429, 0.492, 0.5189873417721519)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, predictions = evaluate(model, dev_dataloader, device)\n",
    "\n",
    "# Calculate and print precision, recall, and F1-score\n",
    "calculate_metrics(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca419d4-c42f-4c8f-a056-1d222c5a656f",
   "metadata": {},
   "source": [
    "Training the CLIP+MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63738088-381e-45ee-9d8b-cf80e44b11bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
