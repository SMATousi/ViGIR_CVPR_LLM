{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afef4df4-daff-47ec-82ea-15c732f6ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd70f09-fc21-423d-9784-47651ec6e3cd",
   "metadata": {},
   "source": [
    "# Defining the Labelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71386fb0-f98d-4e26-9e70-30b5b34cd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava:7b_results_hateful.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava 13b-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'bakllava-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-llama3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-phi3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream(image_name):\n",
    "    root_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'moondream-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a054ab41-d025-4800-97d0-a2661422f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(llava_7b(\"48132.png\"))\n",
    "print(llava_llama3(\"48132.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff94abf-a386-4f28-a46e-835e955e8c47",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3bc313-ce3f-4875-b92f-8ecd100992de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8500 images in the Train set.\n",
      "There are 500 images in the dev set.\n",
      "There are 500 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/simplified_train.json'\n",
    "dev_data_json_path = '/home/macula/SMATousi/CVPR/ViGIR_CVPR_LLM/prompting_framework/prompting_results/hateful/simplified_dev.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for entry in train_data:\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    train_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for entry in dev_data:\n",
    "    Y_dev.append(entry['label'])\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    dev_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5cecc-7c43-4b50-b2a8-6b31b253dc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5b5df0f-fdba-4f11-9d99-e7cfaf24c44f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Applying the LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7ca650-4742-43d6-bb6f-c853dd176269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "lfs = [llava_7b,\n",
    "       llava_13b,\n",
    "       moondream,\n",
    "       llava_llama3,\n",
    "       llava_phi3,\n",
    "       bakllava\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b475d082-aa6a-4fbf-8299-1b1237529a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:06, 72.30it/s]\n",
      "8500it [00:57, 147.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757add37-1ef5-4fc8-937f-c7b963760125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>298</td>\n",
       "      <td>202</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>288</td>\n",
       "      <td>212</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moondream</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>246</td>\n",
       "      <td>254</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>278</td>\n",
       "      <td>222</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.730</td>\n",
       "      <td>267</td>\n",
       "      <td>228</td>\n",
       "      <td>0.539394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.696</td>\n",
       "      <td>276</td>\n",
       "      <td>199</td>\n",
       "      <td>0.581053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "llava_7b      0   [0, 1]      1.00      1.00      0.738      298        202   \n",
       "llava_13b     1   [0, 1]      1.00      1.00      0.738      288        212   \n",
       "moondream     2   [0, 1]      1.00      1.00      0.738      246        254   \n",
       "llava_llama3  3   [0, 1]      1.00      1.00      0.738      278        222   \n",
       "llava_phi3    4   [0, 1]      0.99      0.99      0.730      267        228   \n",
       "bakllava      5   [0, 1]      0.95      0.95      0.696      276        199   \n",
       "\n",
       "              Emp. Acc.  \n",
       "llava_7b       0.596000  \n",
       "llava_13b      0.576000  \n",
       "moondream      0.492000  \n",
       "llava_llama3   0.556000  \n",
       "llava_phi3     0.539394  \n",
       "bakllava       0.581053  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c503ac5-d0cc-4bde-94bf-2ec31cac4aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dd038dc-0729-4e6f-9f14-805066f62eb6",
   "metadata": {},
   "source": [
    "F1 of the labelers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d70b98e-2e01-4b26-8456-9d4d450668b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.59\n",
      "Recall: 0.62\n",
      "F1-score: 0.60\n",
      "Precision: 0.60\n",
      "Recall: 0.45\n",
      "F1-score: 0.52\n",
      "Precision: 0.40\n",
      "Recall: 0.03\n",
      "F1-score: 0.06\n",
      "Precision: 0.54\n",
      "Recall: 0.70\n",
      "F1-score: 0.61\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, f1_score\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(L_dev\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m     precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_dev\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     recall \u001b[38;5;241m=\u001b[39m recall_score(Y_dev, L_dev[:,i])\n\u001b[1;32m      7\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(Y_dev, L_dev[:,i])\n",
      "File \u001b[0;32m~/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2182\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2016\u001b[0m     {\n\u001b[1;32m   2017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2043\u001b[0m ):\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \n\u001b[1;32m   2046\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2182\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2183\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2184\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1767\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \n\u001b[1;32m   1606\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1767\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1556\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1555\u001b[0m             average_options\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1556\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but average=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose another average setting, one of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (y_type, average_options)\n\u001b[1;32m   1559\u001b[0m         )\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1561\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pos_label (set to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) is ignored when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m). You may use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1567\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for i in range(L_dev.shape[1]):\n",
    "    \n",
    "    precision = precision_score(Y_dev, L_dev[:,i])\n",
    "    recall = recall_score(Y_dev, L_dev[:,i])\n",
    "    f1 = f1_score(Y_dev, L_dev[:,i])\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec301f4-aa31-4004-bfc2-b85b2ba88de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d022c-4a6b-4258-86f8-71b615cfa8b3",
   "metadata": {},
   "source": [
    "# Optimization for Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ea9e65-8d70-4621-b3af-f13500eaaf42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|                                                                                              | 0/5000 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=5.341]\n",
      "  6%|█████                                                                             | 311/5000 [00:00<00:02, 1586.26epoch/s]INFO:root:[500 epochs]: TRAIN:[loss=0.002]\n",
      " 14%|███████████▏                                                                      | 684/5000 [00:00<00:01, 2488.42epoch/s]INFO:root:[1000 epochs]: TRAIN:[loss=0.002]\n",
      " 29%|███████████████████████▎                                                         | 1436/5000 [00:00<00:01, 3241.34epoch/s]INFO:root:[1500 epochs]: TRAIN:[loss=0.002]\n",
      " 36%|████████████████████████████▉                                                    | 1786/5000 [00:00<00:00, 3324.92epoch/s]INFO:root:[2000 epochs]: TRAIN:[loss=0.002]\n",
      " 43%|███████████████████████████████████                                              | 2162/5000 [00:00<00:00, 3462.39epoch/s]INFO:root:[2500 epochs]: TRAIN:[loss=0.002]\n",
      " 58%|███████████████████████████████████████████████▎                                 | 2917/5000 [00:00<00:00, 3622.92epoch/s]INFO:root:[3000 epochs]: TRAIN:[loss=0.002]\n",
      " 66%|█████████████████████████████████████████████████████▍                           | 3295/5000 [00:01<00:00, 3669.93epoch/s]INFO:root:[3500 epochs]: TRAIN:[loss=0.002]\n",
      " 73%|███████████████████████████████████████████████████████████▍                     | 3670/5000 [00:01<00:00, 3693.48epoch/s]INFO:root:[4000 epochs]: TRAIN:[loss=0.002]\n",
      " 88%|███████████████████████████████████████████████████████████████████████▌         | 4417/5000 [00:01<00:00, 3711.06epoch/s]INFO:root:[4500 epochs]: TRAIN:[loss=0.002]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3345.43epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427bf3f3-3d55-40e0-938a-a9a672fb88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label model f1 score: 0.5539112050739958\n",
      "Label model roc-auc: 0.605344\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "print(\n",
    "    f\"Label model f1 score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='f1')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Label model roc-auc: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='roc_auc')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eeee78a-8ef5-463e-b837-01e5e2399848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82491001 0.76428977 0.54531989 0.78372908 0.82443517 0.83430104]\n"
     ]
    }
   ],
   "source": [
    "print(label_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c11aac3-a236-48ed-b80f-f97875351d15",
   "metadata": {},
   "source": [
    "# Training the End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85550ff2-272c-4f20-acba-1cddf2ad952b",
   "metadata": {},
   "source": [
    "The dataloader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90e13508-d0fb-4f2c-89da-98c7961f60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom dataset class for loading images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_names, root_dir, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_names (list): List of image file names.\n",
    "            root_dir (string): Directory where images are stored.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_names = image_names\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Build the full path of the image file\n",
    "        img_name = os.path.join(self.root_dir, self.image_names[idx])\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_name).convert('RGB')  # Load image as RGB\n",
    "\n",
    "        # Apply any transformations (e.g., resize, normalization)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f892c82d-e848-41b8-9c58-98462f84401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home1/pupil/goowfd/CVPR_2025/hateful_memes/img/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224 (example)\n",
    "        transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean/std\n",
    "    ])\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_train)\n",
    "label_model_predictions = probs_to_preds(probs_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03c0e90b-c7be-44cf-bfe3-01a361aa2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(image_names=train_image_names, root_dir=root_dir, labels=label_model_predictions, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf71dc2-729a-4ba4-b8f4-33e438edc863",
   "metadata": {},
   "source": [
    "A basic ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f13e49-02cb-4a0d-9e38-ebe167e96c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Pretrained ResNet with MLP head for binary classification\n",
    "class ResNetWithMLP(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNetWithMLP, self).__init__()\n",
    "        # Load a pretrained ResNet (e.g., ResNet18)\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Freeze the ResNet layers if you don't want to train them\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the last fully connected layer with a custom MLP head\n",
    "        num_features = self.resnet.fc.in_features  # Get the number of features in the last layer\n",
    "        self.resnet.fc = MLPHead(input_dim=num_features, output_dim=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17dfac98-4198-4130-9117-060ff4c51175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNetWithMLP(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bcf001b-a515-4e9f-8d4e-2bf5114600b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85337a25-23c3-4bdb-98c6-3f921342c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:27<00:00, 38.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6554, Accuracy: 61.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:17<00:00, 60.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.6356, Accuracy: 64.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 62.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.6321, Accuracy: 64.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.6299, Accuracy: 64.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.6236, Accuracy: 65.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.6240, Accuracy: 64.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.6198, Accuracy: 65.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.6191, Accuracy: 65.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.6137, Accuracy: 65.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 64.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.6127, Accuracy: 66.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Binary classification loss\n",
    "optimizer = optim.Adam(model.resnet.fc.parameters(), lr=0.001)  # Only optimize the MLP parameters\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "train(model, dataloader, criterion, optimizer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea2d27-e855-481d-9df1-490cf74feb7d",
   "metadata": {},
   "source": [
    "Evaluation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd333283-c836-4eac-b7f3-44892acf6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate the model on the dev set\n",
    "def evaluate(model, dev_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels in tqdm(dev_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get predictions (class with the highest score)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Store true labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(labels, predictions):\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7526a040-0b04-4da3-8289-ffb05d17a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = ImageDataset(image_names=dev_image_names, root_dir=root_dir, labels=Y_dev, transform=transform)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d0443e3-46ec-44ac-af0c-06375b1924b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:01<00:00, 31.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5491\n",
      "Recall: 0.4920\n",
      "F1 Score: 0.5190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5491071428571429, 0.492, 0.5189873417721519)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, predictions = evaluate(model, dev_dataloader, device)\n",
    "\n",
    "# Calculate and print precision, recall, and F1-score\n",
    "calculate_metrics(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca419d4-c42f-4c8f-a056-1d222c5a656f",
   "metadata": {},
   "source": [
    "Training the CLIP+MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63738088-381e-45ee-9d8b-cf80e44b11bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
