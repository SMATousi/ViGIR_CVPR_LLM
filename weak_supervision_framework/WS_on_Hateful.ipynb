{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afef4df4-daff-47ec-82ea-15c732f6ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd70f09-fc21-423d-9784-47651ec6e3cd",
   "metadata": {},
   "source": [
    "# Defining the Labelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71386fb0-f98d-4e26-9e70-30b5b34cd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava:7b_results_hateful.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava 13b-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'bakllava-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-llama3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'llava-phi3-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/hateful/total_results/'\n",
    "    llava_7b_results = 'moondream-allsamples-results.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data[image_name] if data[image_name] is not None else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a054ab41-d025-4800-97d0-a2661422f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(llava_7b(\"48132.png\"))\n",
    "print(llava_llama3(\"48132.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff94abf-a386-4f28-a46e-835e955e8c47",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3bc313-ce3f-4875-b92f-8ecd100992de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8500 images in the Train set.\n",
      "There are 500 images in the dev set.\n",
      "There are 500 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_train.json'\n",
    "dev_data_json_path = '../prompting_framework/prompting_results/hateful/simplified_dev.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for entry in train_data:\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    train_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for entry in dev_data:\n",
    "    Y_dev.append(entry['label'])\n",
    "    img_name, ext = entry['img'].split('.')\n",
    "    padded_img_name = img_name.zfill(5)  # Pad the image name to 5 digits\n",
    "    dev_image_names.append(f\"{padded_img_name}.{ext}\")\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5cecc-7c43-4b50-b2a8-6b31b253dc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5b5df0f-fdba-4f11-9d99-e7cfaf24c44f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Applying the LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7ca650-4742-43d6-bb6f-c853dd176269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "lfs = [llava_7b,\n",
    "       llava_13b,\n",
    "       moondream,\n",
    "       llava_llama3,\n",
    "       llava_phi3,\n",
    "       bakllava\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b475d082-aa6a-4fbf-8299-1b1237529a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:03, 164.46it/s]\n",
      "8500it [00:54, 155.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757add37-1ef5-4fc8-937f-c7b963760125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>298</td>\n",
       "      <td>202</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>288</td>\n",
       "      <td>212</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moondream</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>246</td>\n",
       "      <td>254</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.738</td>\n",
       "      <td>278</td>\n",
       "      <td>222</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.730</td>\n",
       "      <td>267</td>\n",
       "      <td>228</td>\n",
       "      <td>0.539394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.696</td>\n",
       "      <td>276</td>\n",
       "      <td>199</td>\n",
       "      <td>0.581053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "llava_7b      0   [0, 1]      1.00      1.00      0.738      298        202   \n",
       "llava_13b     1   [0, 1]      1.00      1.00      0.738      288        212   \n",
       "moondream     2   [0, 1]      1.00      1.00      0.738      246        254   \n",
       "llava_llama3  3   [0, 1]      1.00      1.00      0.738      278        222   \n",
       "llava_phi3    4   [0, 1]      0.99      0.99      0.730      267        228   \n",
       "bakllava      5   [0, 1]      0.95      0.95      0.696      276        199   \n",
       "\n",
       "              Emp. Acc.  \n",
       "llava_7b       0.596000  \n",
       "llava_13b      0.576000  \n",
       "moondream      0.492000  \n",
       "llava_llama3   0.556000  \n",
       "llava_phi3     0.539394  \n",
       "bakllava       0.581053  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd038dc-0729-4e6f-9f14-805066f62eb6",
   "metadata": {},
   "source": [
    "F1 of the labelers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11214f33-2540-4854-a076-f581ca68b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered)\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered, average='weighted')\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered, average='weighted')\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered, average='weighted')\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d70b98e-2e01-4b26-8456-9d4d450668b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Confusion Matrix: [[144 106]\n",
      " [ 96 154]]\n",
      "Precision: 0.5961538461538461\n",
      "Recall: 0.596\n",
      "F1 Score: 0.5958383353341337\n",
      "Accuracy: 0.596\n",
      "-------------------------------\n",
      "Confusion Matrix: [[175  75]\n",
      " [137 113]]\n",
      "Precision: 0.5809806328423349\n",
      "Recall: 0.576\n",
      "F1 Score: 0.5693787679357806\n",
      "Accuracy: 0.576\n",
      "-------------------------------\n",
      "Confusion Matrix: [[238  12]\n",
      " [242   8]]\n",
      "Precision: 0.4479166666666667\n",
      "Recall: 0.492\n",
      "F1 Score: 0.3556570268899036\n",
      "Accuracy: 0.492\n",
      "-------------------------------\n",
      "Confusion Matrix: [[103 147]\n",
      " [ 75 175]]\n",
      "Precision: 0.5610649731314118\n",
      "Recall: 0.556\n",
      "F1 Score: 0.5465982615515326\n",
      "Accuracy: 0.556\n",
      "-------------------------------\n",
      "Confusion Matrix: [[143 107]\n",
      " [121 124]]\n",
      "Precision: 0.5392561983471075\n",
      "Recall: 0.5393939393939394\n",
      "F1 Score: 0.538893166038966\n",
      "Accuracy: 0.5393939393939394\n",
      "-------------------------------\n",
      "Confusion Matrix: [[164  72]\n",
      " [127 112]]\n",
      "Precision: 0.5862772574645938\n",
      "Recall: 0.5810526315789474\n",
      "F1 Score: 0.5756776804810957\n",
      "Accuracy: 0.5810526315789474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for i in range(L_dev.shape[1]):\n",
    "\n",
    "    print(\"-------------------------------\")\n",
    "    metrics = calculate_metrics(Y_dev, L_dev[:,i])\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec301f4-aa31-4004-bfc2-b85b2ba88de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d022c-4a6b-4258-86f8-71b615cfa8b3",
   "metadata": {},
   "source": [
    "# Optimization for Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ea9e65-8d70-4621-b3af-f13500eaaf42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|                                                                                                                               | 0/5000 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=5.341]\n",
      "  7%|████████▌                                                                                                          | 370/5000 [00:00<00:02, 2106.79epoch/s]INFO:root:[500 epochs]: TRAIN:[loss=0.002]\n",
      " 15%|█████████████████                                                                                                  | 743/5000 [00:00<00:01, 2833.66epoch/s]INFO:root:[1000 epochs]: TRAIN:[loss=0.002]\n",
      " 30%|██████████████████████████████████                                                                                | 1496/5000 [00:00<00:01, 3396.96epoch/s]INFO:root:[1500 epochs]: TRAIN:[loss=0.002]\n",
      " 37%|██████████████████████████████████████████▋                                                                       | 1874/5000 [00:00<00:00, 3524.25epoch/s]INFO:root:[2000 epochs]: TRAIN:[loss=0.002]\n",
      " 45%|███████████████████████████████████████████████████▎                                                              | 2251/5000 [00:00<00:00, 3601.77epoch/s]INFO:root:[2500 epochs]: TRAIN:[loss=0.002]\n",
      " 53%|███████████████████████████████████████████████████████████▉                                                      | 2631/5000 [00:00<00:00, 3664.26epoch/s]INFO:root:[3000 epochs]: TRAIN:[loss=0.002]\n",
      " 68%|█████████████████████████████████████████████████████████████████████████████▍                                    | 3394/5000 [00:01<00:00, 3739.72epoch/s]INFO:root:[3500 epochs]: TRAIN:[loss=0.002]\n",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████                            | 3772/5000 [00:01<00:00, 3751.72epoch/s]INFO:root:[4000 epochs]: TRAIN:[loss=0.002]\n",
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████▋                   | 4154/5000 [00:01<00:00, 3769.88epoch/s]INFO:root:[4500 epochs]: TRAIN:[loss=0.002]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3497.49epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427bf3f3-3d55-40e0-938a-a9a672fb88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[158  92]\n",
      " [119 131]]\n",
      "Precision: 0.5789205290508491\n",
      "Recall: 0.578\n",
      "F1 Score: 0.5767658492163148\n",
      "Accuracy: 0.578\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeee78a-8ef5-463e-b837-01e5e2399848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c11aac3-a236-48ed-b80f-f97875351d15",
   "metadata": {},
   "source": [
    "# Training the End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85550ff2-272c-4f20-acba-1cddf2ad952b",
   "metadata": {},
   "source": [
    "The dataloader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90e13508-d0fb-4f2c-89da-98c7961f60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom dataset class for loading images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_names, root_dir, labels, target_dists, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_names (list): List of image file names.\n",
    "            root_dir (string): Directory where images are stored.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_names = image_names\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_dists = target_dists\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Build the full path of the image file\n",
    "        img_name = os.path.join(self.root_dir, self.image_names[idx])\n",
    "        label = self.labels[idx]\n",
    "        target_dist = self.target_dists[idx]\n",
    "        image = Image.open(img_name).convert('RGB')  # Load image as RGB\n",
    "\n",
    "        # Apply any transformations (e.g., resize, normalization)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label), torch.tensor(target_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f892c82d-e848-41b8-9c58-98462f84401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home1/pupil/goowfd/CVPR_2025/hateful_memes/img/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224 (example)\n",
    "        transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean/std\n",
    "    ])\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_train)\n",
    "label_model_predictions = probs_to_preds(probs_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03c0e90b-c7be-44cf-bfe3-01a361aa2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(image_names=train_image_names, \n",
    "                       root_dir=root_dir, \n",
    "                       labels=label_model_predictions, \n",
    "                       target_dists=probs_dev, \n",
    "                       transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf71dc2-729a-4ba4-b8f4-33e438edc863",
   "metadata": {},
   "source": [
    "A basic ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f13e49-02cb-4a0d-9e38-ebe167e96c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def expected_cross_entropy_loss(logits, target_distributions):\n",
    "    \"\"\"\n",
    "    Computes the expected cross-entropy loss for a batch of predictions and target distributions.\n",
    "\n",
    "    Parameters:\n",
    "    logits (torch.Tensor): The raw output from the model of shape (batch_size, num_classes).\n",
    "    target_distributions (torch.Tensor): The target class distributions of shape (batch_size, num_classes),\n",
    "                                         where each row is a probability distribution over classes.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The expected cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    \n",
    "    # Compute the element-wise product between target distributions and log probabilities\n",
    "    # Then, sum across classes to get the cross-entropy for each instance\n",
    "    cross_entropy = -torch.sum(target_distributions * log_probs, dim=1)\n",
    "    \n",
    "    # Take the mean over the batch\n",
    "    loss = cross_entropy.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Pretrained ResNet with MLP head for binary classification\n",
    "class ResNetWithMLP(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNetWithMLP, self).__init__()\n",
    "        # Load a pretrained ResNet (e.g., ResNet18)\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Freeze the ResNet layers if you don't want to train them\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the last fully connected layer with a custom MLP head\n",
    "        num_features = self.resnet.fc.in_features  # Get the number of features in the last layer\n",
    "        self.resnet.fc = MLPHead(input_dim=num_features, output_dim=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17dfac98-4198-4130-9117-060ff4c51175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNetWithMLP(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bcf001b-a515-4e9f-8d4e-2bf5114600b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels, target_dist in tqdm(train_loader):\n",
    "            images, labels, target_dist = images.to(device), labels.to(device), target_dist.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "            # loss = criterion(outputs, labels)  # Compute the loss\n",
    "            loss = expected_cross_entropy_loss(outputs, target_dist)\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85337a25-23c3-4bdb-98c6-3f921342c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:30<00:00, 35.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6691, Accuracy: 59.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:26<00:00, 40.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.6468, Accuracy: 63.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:21<00:00, 50.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.6386, Accuracy: 64.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:18<00:00, 58.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.6370, Accuracy: 64.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:17<00:00, 59.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.6361, Accuracy: 65.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:17<00:00, 61.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.6334, Accuracy: 65.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 63.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.6260, Accuracy: 65.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 62.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.6278, Accuracy: 65.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:16<00:00, 62.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.6271, Accuracy: 65.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:17<00:00, 62.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.6203, Accuracy: 66.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Binary classification loss\n",
    "optimizer = optim.Adam(model.resnet.fc.parameters(), lr=0.001)  # Only optimize the MLP parameters\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "train(model, dataloader, criterion, optimizer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea2d27-e855-481d-9df1-490cf74feb7d",
   "metadata": {},
   "source": [
    "Evaluation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd333283-c836-4eac-b7f3-44892acf6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate the model on the dev set\n",
    "def evaluate(model, dev_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels, target_dist in tqdm(dev_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get predictions (class with the highest score)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Store true labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(labels, predictions):\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7526a040-0b04-4da3-8289-ffb05d17a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = ImageDataset(image_names=dev_image_names, root_dir=root_dir, labels=Y_dev, target_dists=probs_dev, transform=transform)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d0443e3-46ec-44ac-af0c-06375b1924b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:01<00:00, 55.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5355\n",
      "Recall: 0.4520\n",
      "F1 Score: 0.4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate and print precision, recall, and F1-score\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# calculate_metrics(labels, predictions)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m metrics \u001b[38;5;241m=\u001b[39m calculate_metrics(labels, predictions)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "labels, predictions = evaluate(model, dev_dataloader, device)\n",
    "\n",
    "# Calculate and print precision, recall, and F1-score\n",
    "# calculate_metrics(labels, predictions)\n",
    "metrics = calculate_metrics(labels, predictions)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca419d4-c42f-4c8f-a056-1d222c5a656f",
   "metadata": {},
   "source": [
    "Training the CLIP+MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63738088-381e-45ee-9d8b-cf80e44b11bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
