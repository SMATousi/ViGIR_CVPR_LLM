{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9edf38-ca84-4d5a-b995-d5eaaf5e2ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b90671d-e392-45bf-b734-529af42ca52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/DTD/interpreter/'\n",
    "    llava_7b_results = 'llava7b_dtd.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/DTD/interpreter'\n",
    "    llava_7b_results = 'llava13b_dtd.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/DTD/interpreter'\n",
    "    llava_7b_results = 'bakllava_dtd.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/DTD/interpreter'\n",
    "    llava_7b_results = 'llava_llama3_dtd.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)\n",
    "\n",
    "# @labeling_function()\n",
    "# def llava_phi3(image_name):\n",
    "#     root_path = '../prompting_framework/prompting_results/DTD/interpreter'\n",
    "#     llava_7b_results = 'eurosat_llava_phi3.json'\n",
    "#     path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "#     with open(path_to_llava_7b_results, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "\n",
    "#     return data.get(image_name, -1)\n",
    "\n",
    "\n",
    "# @labeling_function()\n",
    "# def moondream(image_name):\n",
    "#     root_path = '../prompting_framework/prompting_results/DTD/interpreter'\n",
    "#     llava_7b_results = 'eurosat_moondream.json'\n",
    "#     path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "#     with open(path_to_llava_7b_results, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "\n",
    "#     return data.get(image_name, -1)\n",
    "\n",
    "# @labeling_function()\n",
    "# def llava_34b(image_name):\n",
    "#     root_path = '../prompting_framework/prompting_results/DTD/interpreter'\n",
    "#     llava_7b_results = 'eurosat_llava34b.json'\n",
    "#     path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "#     with open(path_to_llava_7b_results, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "\n",
    "#     return data.get(image_name, -1)\n",
    "\n",
    "@labeling_function()\n",
    "def minicpm(image_name):\n",
    "    root_path = '../prompting_framework/prompting_results/DTD/interpreter'\n",
    "    llava_7b_results = 'minicpm_dtd.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(image_name, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f903b6a9-97f4-49d1-840a-71e18a886497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2820 images in the Train set.\n",
      "There are 1692 images in the dev set.\n",
      "There are 1692 labels in the dev set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../prompting_framework/prompting_results/DTD/interpreter/train_gt.json'\n",
    "dev_data_json_path = '../prompting_framework/prompting_results/DTD/interpreter/test_gt.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for item in train_data:\n",
    "    train_image_names.append(item)\n",
    "\n",
    "with open(dev_data_json_path, 'r') as file:\n",
    "    dev_data = json.load(file)\n",
    "    \n",
    "dev_image_names = []\n",
    "Y_dev = []\n",
    "for item in dev_data:\n",
    "    Y_dev.append(dev_data[item])\n",
    "    dev_image_names.append(item)\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "print(f\"There are {len(dev_image_names)} images in the dev set.\")\n",
    "print(f\"There are {len(Y_dev)} labels in the dev set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9112d9-ea2d-44a1-822a-49afe590a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "list_of_all_the_models = ['llava_13b',\n",
    "       'llava_7b',\n",
    "       'llava_llama3',\n",
    "       'minicpm',\n",
    "       'bakllava'\n",
    "       ]\n",
    "\n",
    "lfs = [llava_13b,\n",
    "       llava_7b,\n",
    "       llava_llama3,\n",
    "       minicpm,\n",
    "       bakllava\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68ca8d0-d969-4b1c-9384-880c22d7e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1692it [00:03, 509.22it/s]\n",
      "2820it [00:05, 509.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_dev = applier.apply(dev_image_names)\n",
    "L_train = applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18151ef-87f5-47c5-a8d9-8dbc294c651d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_13b</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...</td>\n",
       "      <td>0.843381</td>\n",
       "      <td>0.841608</td>\n",
       "      <td>0.645390</td>\n",
       "      <td>90</td>\n",
       "      <td>1337</td>\n",
       "      <td>0.063069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_7b</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>0.785461</td>\n",
       "      <td>0.772459</td>\n",
       "      <td>0.582742</td>\n",
       "      <td>94</td>\n",
       "      <td>1235</td>\n",
       "      <td>0.070730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...</td>\n",
       "      <td>0.920213</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.677305</td>\n",
       "      <td>61</td>\n",
       "      <td>1496</td>\n",
       "      <td>0.039178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicpm</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>0.783688</td>\n",
       "      <td>0.782506</td>\n",
       "      <td>0.588061</td>\n",
       "      <td>99</td>\n",
       "      <td>1227</td>\n",
       "      <td>0.074661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 19, 24...</td>\n",
       "      <td>0.262411</td>\n",
       "      <td>0.258865</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>34</td>\n",
       "      <td>410</td>\n",
       "      <td>0.076577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              j                                           Polarity  Coverage  \\\n",
       "llava_13b     0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...  0.843381   \n",
       "llava_7b      1  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  0.785461   \n",
       "llava_llama3  2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...  0.920213   \n",
       "minicpm       3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  0.783688   \n",
       "bakllava      4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 19, 24...  0.262411   \n",
       "\n",
       "              Overlaps  Conflicts  Correct  Incorrect  Emp. Acc.  \n",
       "llava_13b     0.841608   0.645390       90       1337   0.063069  \n",
       "llava_7b      0.772459   0.582742       94       1235   0.070730  \n",
       "llava_llama3  0.893617   0.677305       61       1496   0.039178  \n",
       "minicpm       0.782506   0.588061       99       1227   0.074661  \n",
       "bakllava      0.258865   0.148936       34        410   0.076577  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev = np.array(Y_dev)\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9f2d2c-9771-442d-a316-fce009516bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2d3d1e-6b32-4c78-8ac1-49bd650ddc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llava_13b</td>\n",
       "      <td>0.068654</td>\n",
       "      <td>0.076074</td>\n",
       "      <td>0.063069</td>\n",
       "      <td>0.053394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llava_7b</td>\n",
       "      <td>0.057107</td>\n",
       "      <td>0.063121</td>\n",
       "      <td>0.070730</td>\n",
       "      <td>0.050743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llava_llama3</td>\n",
       "      <td>0.037444</td>\n",
       "      <td>0.037216</td>\n",
       "      <td>0.039178</td>\n",
       "      <td>0.030148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>minicpm</td>\n",
       "      <td>0.074856</td>\n",
       "      <td>0.075287</td>\n",
       "      <td>0.074661</td>\n",
       "      <td>0.066896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bakllava</td>\n",
       "      <td>0.040243</td>\n",
       "      <td>0.020065</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.021253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model    Recall  Precision  Accuracy  F1 Score\n",
       "0     llava_13b  0.068654   0.076074  0.063069  0.053394\n",
       "1      llava_7b  0.057107   0.063121  0.070730  0.050743\n",
       "2  llava_llama3  0.037444   0.037216  0.039178  0.030148\n",
       "3       minicpm  0.074856   0.075287  0.074661  0.066896\n",
       "4      bakllava  0.040243   0.020065  0.076577  0.021253"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "# Example ground truth and predictions for six models\n",
    "# Replace these arrays with actual predictions from each model\n",
    "y_true = Y_dev\n",
    "predictions = {}\n",
    "\n",
    "for i in range(L_dev.shape[1]):\n",
    "    predictions[list_of_all_the_models[i]] = L_dev[:,i]\n",
    "    \n",
    "# Create a DataFrame to store confusion matrix results and metrics\n",
    "confusion_data = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    # Confusion Matrix\n",
    "    metrics = calculate_metrics(Y_dev, y_pred)\n",
    "\n",
    "    precision = metrics['Precision']\n",
    "    recall = metrics['Recall']\n",
    "    f1 = metrics['F1 Score']\n",
    "    accuracy = metrics['Accuracy']\n",
    "    # Append data\n",
    "    confusion_data.append([\n",
    "        model_name,\n",
    "        recall, precision, accuracy, f1\n",
    "    ])\n",
    "\n",
    "# Convert to a DataFrame for display\n",
    "confusion_df = pd.DataFrame(confusion_data, columns=[\n",
    "    'Model', \n",
    "    'Recall', 'Precision', 'Accuracy', 'F1 Score'\n",
    "])\n",
    "\n",
    "# Display the table with confusion matrix and metrics\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52301d68-0e50-4cad-a490-5ed2d28a15b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 855.02epoch/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=47, verbose=False)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e412f912-52a5-46f9-9118-0f43fba2a872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.08448938981583165\n",
      "Recall: 0.060874704491725766\n",
      "F1 Score: 0.057463189697828994\n",
      "Accuracy: 0.060874704491725766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667bf15-3553-4ee6-9b6b-745bb62a2809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
