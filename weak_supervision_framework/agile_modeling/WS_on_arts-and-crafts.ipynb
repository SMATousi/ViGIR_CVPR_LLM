{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b708cc2-ca3f-4066-9d58-ea3c515ea29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "885b51c8-181d-43f4-a883-b980f9359eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores=None, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "    # y_scores_fileterd = y_scores[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    if y_scores is not None:\n",
    "        precision_list, recall_list, threshold = precision_recall_curve(y_true, y_scores)\n",
    "        auc_score = auc(recall_list, precision_list)\n",
    "    else:\n",
    "        precision_list, recall_list, threshold = precision_recall_curve(y_true, y_pred)\n",
    "        auc_score = auc(recall_list, precision_list)\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered)\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered)\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered)\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered)\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'auc': auc_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4c696a1-7309-4ac9-b31c-99f1208ebd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "dataset_name = \"arts-and-crafts\"\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b_test(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava_7b-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b_train(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava_7b-train.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b_test(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava_13b-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b_train(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava_13b-train.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "        \n",
    "@labeling_function()\n",
    "def bakllava_test(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-bakllava-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava_train(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-bakllava-train.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "        \n",
    "@labeling_function()\n",
    "def llava_llama3_test(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava-llama3-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3_train(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava-llama3-train.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "        \n",
    "@labeling_function()\n",
    "def llava_phi3_test(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava-phi3-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3_train(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava-phi3-train.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "        \n",
    "\n",
    "@labeling_function()\n",
    "def moondream_test(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-moondream-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def moondream_train(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-moondream-train.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llama_3_2_vision_test(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llama3.2-vision_11b-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llama_3_2_vision_train(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llama3.2-vision_11b-train.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_34b_test(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava_34b-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_34b_train(image_name):\n",
    "    root_path = f'../../prompting_framework/prompting_results/agile_datasets/{dataset_name}/'\n",
    "    llava_7b_results = f'{dataset_name}-llava_34b-train.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return 1-data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebdb744b-9781-44f3-83c4-aa0362d6fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# print(llava_34b_test('3'))\n",
    "print(llava_llama3_test('4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b05b82e1-3218-43a0-a62e-1a708680aaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2551 images in the Train set.\n",
      "There are 599 images in the test set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../../prompting_framework/prompting_results/agile_datasets/arts-and-crafts/arts-and-crafts-llava_13b-train-raw_info.json'\n",
    "test_data_json_path = '../../prompting_framework/prompting_results/agile_datasets/arts-and-crafts/arts-and-crafts-llava-phi3-test-raw_info.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "\n",
    "with open(test_data_json_path, 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "Y_train = []\n",
    "for item in train_data:\n",
    "    train_image_names.append(item)\n",
    "    Y_train.append(train_data[item][\"label\"])\n",
    "\n",
    "\n",
    "test_image_names = []\n",
    "Y_test = []\n",
    "for item in test_data:\n",
    "    test_image_names.append(item)\n",
    "    Y_test.append(test_data[item][\"label\"])\n",
    "\n",
    "# with open(dev_data_json_path, 'r') as file:\n",
    "#     dev_data = json.load(file)\n",
    "    \n",
    "# dev_image_names = []\n",
    "# Y_dev = []\n",
    "# for item in dev_data:\n",
    "#     Y_dev.append(dev_data[item])\n",
    "#     dev_image_names.append(item)\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "\n",
    "print(f\"There are {len(test_image_names)} images in the test set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a42ab7c6-63e2-44a2-b96f-8ed0d9343ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "list_of_all_the_models = [\n",
    "    'llava_13b_test',\n",
    "       'llava_7b_test',\n",
    "       'llava_34b_test',\n",
    "       'llava_llama3_test',\n",
    "       'bakllava_test',\n",
    "       'llama_3_2_vision_test',\n",
    "       'llava_phi3_test',\n",
    "       'moondream_test'\n",
    "       ]\n",
    "\n",
    "test_lfs = [\n",
    "        llava_13b_test,\n",
    "       llava_7b_test,\n",
    "       llava_34b_test,\n",
    "       llava_llama3_test,\n",
    "       bakllava_test,\n",
    "       llama_3_2_vision_test,\n",
    "       llava_phi3_test,\n",
    "       moondream_test\n",
    "       ]\n",
    "\n",
    "train_lfs = [\n",
    "        llava_13b_train,\n",
    "       llava_7b_train,\n",
    "       llava_34b_train,\n",
    "       llava_llama3_train,\n",
    "       bakllava_train,\n",
    "       llama_3_2_vision_train,\n",
    "       llava_phi3_train,\n",
    "       moondream_train\n",
    "       ]\n",
    "\n",
    "test_applier = LFApplier(test_lfs)\n",
    "train_applier = LFApplier(train_lfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c19fd18-0e7d-4a07-a7cf-bc14a43b868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599it [00:03, 162.32it/s]\n",
      "2551it [01:05, 39.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_test = test_applier.apply(test_image_names)\n",
    "L_train = train_applier.apply(train_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99ac222b-b53e-42be-8662-21e12d54ee36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_13b_test</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.864775</td>\n",
       "      <td>417</td>\n",
       "      <td>165</td>\n",
       "      <td>0.716495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_7b_test</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.864775</td>\n",
       "      <td>280</td>\n",
       "      <td>302</td>\n",
       "      <td>0.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_34b_test</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.946578</td>\n",
       "      <td>0.946578</td>\n",
       "      <td>0.839733</td>\n",
       "      <td>404</td>\n",
       "      <td>163</td>\n",
       "      <td>0.712522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3_test</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.969950</td>\n",
       "      <td>0.969950</td>\n",
       "      <td>0.863105</td>\n",
       "      <td>406</td>\n",
       "      <td>175</td>\n",
       "      <td>0.698795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava_test</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.864775</td>\n",
       "      <td>386</td>\n",
       "      <td>196</td>\n",
       "      <td>0.663230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_3_2_vision_test</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.964942</td>\n",
       "      <td>0.964942</td>\n",
       "      <td>0.858097</td>\n",
       "      <td>338</td>\n",
       "      <td>240</td>\n",
       "      <td>0.584775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3_test</th>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.864775</td>\n",
       "      <td>341</td>\n",
       "      <td>241</td>\n",
       "      <td>0.585911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moondream_test</th>\n",
       "      <td>7</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.971619</td>\n",
       "      <td>0.864775</td>\n",
       "      <td>384</td>\n",
       "      <td>198</td>\n",
       "      <td>0.659794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "llava_13b_test         0   [0, 1]  0.971619  0.971619   0.864775      417   \n",
       "llava_7b_test          1   [0, 1]  0.971619  0.971619   0.864775      280   \n",
       "llava_34b_test         2   [0, 1]  0.946578  0.946578   0.839733      404   \n",
       "llava_llama3_test      3   [0, 1]  0.969950  0.969950   0.863105      406   \n",
       "bakllava_test          4   [0, 1]  0.971619  0.971619   0.864775      386   \n",
       "llama_3_2_vision_test  5   [0, 1]  0.964942  0.964942   0.858097      338   \n",
       "llava_phi3_test        6   [0, 1]  0.971619  0.971619   0.864775      341   \n",
       "moondream_test         7   [0, 1]  0.971619  0.971619   0.864775      384   \n",
       "\n",
       "                       Incorrect  Emp. Acc.  \n",
       "llava_13b_test               165   0.716495  \n",
       "llava_7b_test                302   0.481100  \n",
       "llava_34b_test               163   0.712522  \n",
       "llava_llama3_test            175   0.698795  \n",
       "bakllava_test                196   0.663230  \n",
       "llama_3_2_vision_test        240   0.584775  \n",
       "llava_phi3_test              241   0.585911  \n",
       "moondream_test               198   0.659794  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_numerical = []\n",
    "for i in range(len(Y_test)):\n",
    "    if Y_test[i] == 'Yes':\n",
    "        Y_test_numerical.append(1)\n",
    "    elif Y_test[i] == 'No':\n",
    "        Y_test_numerical.append(0)\n",
    "\n",
    "Y_test_numerical = np.array(Y_test_numerical)\n",
    "\n",
    "Y_train_numerical = []\n",
    "for i in range(len(Y_train)):\n",
    "    if Y_train[i] == 'Yes':\n",
    "        Y_train_numerical.append(1)\n",
    "    elif Y_train[i] == 'No':\n",
    "        Y_train_numerical.append(0)\n",
    "\n",
    "Y_train_numerical = np.array(Y_train_numerical)\n",
    "\n",
    "LFAnalysis(L_test, test_lfs).lf_summary(Y_test_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6888712c-edb8-40cb-80d2-cbb557801e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llava_13b_test</td>\n",
       "      <td>77</td>\n",
       "      <td>120</td>\n",
       "      <td>45</td>\n",
       "      <td>340</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>0.804734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llava_7b_test</td>\n",
       "      <td>187</td>\n",
       "      <td>10</td>\n",
       "      <td>292</td>\n",
       "      <td>93</td>\n",
       "      <td>0.241558</td>\n",
       "      <td>0.902913</td>\n",
       "      <td>0.481100</td>\n",
       "      <td>0.381148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llava_34b_test</td>\n",
       "      <td>84</td>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>320</td>\n",
       "      <td>0.844327</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.712522</td>\n",
       "      <td>0.797011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llava_llama3_test</td>\n",
       "      <td>78</td>\n",
       "      <td>118</td>\n",
       "      <td>57</td>\n",
       "      <td>328</td>\n",
       "      <td>0.851948</td>\n",
       "      <td>0.735426</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.789410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bakllava_test</td>\n",
       "      <td>105</td>\n",
       "      <td>92</td>\n",
       "      <td>104</td>\n",
       "      <td>281</td>\n",
       "      <td>0.729870</td>\n",
       "      <td>0.753351</td>\n",
       "      <td>0.663230</td>\n",
       "      <td>0.741425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama_3_2_vision_test</td>\n",
       "      <td>178</td>\n",
       "      <td>18</td>\n",
       "      <td>222</td>\n",
       "      <td>160</td>\n",
       "      <td>0.418848</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.584775</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llava_phi3_test</td>\n",
       "      <td>147</td>\n",
       "      <td>50</td>\n",
       "      <td>191</td>\n",
       "      <td>194</td>\n",
       "      <td>0.503896</td>\n",
       "      <td>0.795082</td>\n",
       "      <td>0.585911</td>\n",
       "      <td>0.616852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>moondream_test</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>384</td>\n",
       "      <td>0.997403</td>\n",
       "      <td>0.660929</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.795031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  True Negative  False Positive  False Negative  \\\n",
       "0         llava_13b_test             77             120              45   \n",
       "1          llava_7b_test            187              10             292   \n",
       "2         llava_34b_test             84             104              59   \n",
       "3      llava_llama3_test             78             118              57   \n",
       "4          bakllava_test            105              92             104   \n",
       "5  llama_3_2_vision_test            178              18             222   \n",
       "6        llava_phi3_test            147              50             191   \n",
       "7         moondream_test              0             197               1   \n",
       "\n",
       "   True Positive    Recall  Precision  Accuracy  F1 Score  \n",
       "0            340  0.883117   0.739130  0.716495  0.804734  \n",
       "1             93  0.241558   0.902913  0.481100  0.381148  \n",
       "2            320  0.844327   0.754717  0.712522  0.797011  \n",
       "3            328  0.851948   0.735426  0.698795  0.789410  \n",
       "4            281  0.729870   0.753351  0.663230  0.741425  \n",
       "5            160  0.418848   0.898876  0.584775  0.571429  \n",
       "6            194  0.503896   0.795082  0.585911  0.616852  \n",
       "7            384  0.997403   0.660929  0.659794  0.795031  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "# Example ground truth and predictions for six models\n",
    "# Replace these arrays with actual predictions from each model\n",
    "y_true = Y_test_numerical\n",
    "predictions = {}\n",
    "\n",
    "for i in range(L_test.shape[1]):\n",
    "    predictions[list_of_all_the_models[i]] = L_test[:,i]\n",
    "    \n",
    "# Create a DataFrame to store confusion matrix results and metrics\n",
    "confusion_data = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    # Confusion Matrix\n",
    "    metrics = calculate_metrics(Y_test_numerical, y_pred)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['Confusion Matrix'].ravel()\n",
    "    precision = metrics['Precision']\n",
    "    recall = metrics['Recall']\n",
    "    f1 = metrics['F1 Score']\n",
    "    accuracy = metrics['Accuracy']\n",
    "    # Append data\n",
    "    confusion_data.append([\n",
    "        model_name, tn, fp, fn, tp, \n",
    "        recall, precision, accuracy, f1\n",
    "    ])\n",
    "\n",
    "# Convert to a DataFrame for display\n",
    "confusion_df = pd.DataFrame(confusion_data, columns=[\n",
    "    'Model', 'True Negative', 'False Positive', 'False Negative', 'True Positive', \n",
    "    'Recall', 'Precision', 'Accuracy', 'F1 Score'\n",
    "])\n",
    "\n",
    "# Display the table with confusion matrix and metrics\n",
    "confusion_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70ab87c2-9b8d-410d-8e4d-e85b0380c0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llava_13b_test',\n",
       " 'llava_7b_test',\n",
       " 'llava_34b_test',\n",
       " 'llava_llama3_test',\n",
       " 'bakllava_test',\n",
       " 'llama_3_2_vision_test',\n",
       " 'llava_phi3_test',\n",
       " 'moondream_test']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_all_the_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784b16e-b417-4ac3-823c-48f6a84723f5",
   "metadata": {},
   "source": [
    "# Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9da9640a-b01c-42b3-924c-5c58ae943827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(labels):\n",
    "    # Assuming the labels are categorical and using mode to find the most frequent label\n",
    "    from scipy.stats import mode\n",
    "    # Using mode along axis=1 to find the most common element across columns\n",
    "    modes = mode(labels, axis=1)\n",
    "    # modes.mode contains the most common values, reshaping to (500,) for a clean 1D array output\n",
    "    return modes.mode.reshape(-1)\n",
    "\n",
    "# Applying the majority vote function\n",
    "majority_labels_test = majority_vote(L_test)\n",
    "majority_labels_train = majority_vote(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7bb5b28-a9de-4f80-a81a-94f6f79a76d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[124  73]\n",
      " [ 95 290]]\n",
      "Precision: 0.7988980716253443\n",
      "Recall: 0.7532467532467533\n",
      "F1 Score: 0.7754010695187166\n",
      "Accuracy: 0.711340206185567\n",
      "auc: 0.8517768261247454\n",
      "Confusion Matrix: [[ 559  482]\n",
      " [ 268 1181]]\n",
      "Precision: 0.7101623571858088\n",
      "Recall: 0.8150448585231194\n",
      "F1 Score: 0.7589974293059126\n",
      "Accuracy: 0.6987951807228916\n",
      "auc: 0.8105875921246289\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(Y_test_numerical, majority_labels_test)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "metrics = calculate_metrics(Y_train_numerical, majority_labels_train)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8942e-288a-417c-aec7-a6ae1ffcb6a1",
   "metadata": {},
   "source": [
    "# Snorkel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4fb7ca0-30a8-42ef-b790-6d490270aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 1687.89epoch/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=False)\n",
    "label_model.fit(L_train, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95b5fa4a-0580-4ead-b5b9-d1748cd62ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[131  71]\n",
      " [129 268]]\n",
      "Precision: 0.7905604719764012\n",
      "Recall: 0.6750629722921915\n",
      "F1 Score: 0.7282608695652174\n",
      "Accuracy: 0.666110183639399\n",
      "auc: 0.8387528136416879\n",
      "Confusion Matrix: [[ 619  446]\n",
      " [ 345 1141]]\n",
      "Precision: 0.7189666036546944\n",
      "Recall: 0.7678331090174967\n",
      "F1 Score: 0.7425968109339408\n",
      "Accuracy: 0.6899255194041553\n",
      "auc: 0.8055036572257317\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_test)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "probs_train = label_model.predict_proba(L_train)\n",
    "preds_train = probs_to_preds(probs_train)\n",
    "\n",
    "metrics = calculate_metrics(Y_test_numerical, preds_dev, probs_dev[:,1])\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "metrics = calculate_metrics(Y_train_numerical, preds_train, probs_train[:,1])\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ebc148c9-37e2-4d25-acc3-f123213c2aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.99998183, 0.97182417, ..., 0.97182417, 0.65899334,\n",
       "       0.99999999])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(probs_train, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75df254-9b5f-40c8-a1af-d42f2cf8edc0",
   "metadata": {},
   "source": [
    "# Hyper Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dd53a2d-93e4-4e1c-9ee8-82d6ed89d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperlm import HyperLabelModel\n",
    "hlm = HyperLabelModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24035d7b-095a-4a07-aa9b-6c9e21963521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[124  78]\n",
      " [ 98 299]]\n",
      "Precision: 0.7931034482758621\n",
      "Recall: 0.7531486146095718\n",
      "F1 Score: 0.772609819121447\n",
      "Accuracy: 0.7061769616026711\n",
      "auc: 0.8549290364510642\n",
      "Confusion Matrix: [[ 559  506]\n",
      " [ 268 1218]]\n",
      "Precision: 0.7064965197215777\n",
      "Recall: 0.819650067294751\n",
      "F1 Score: 0.7588785046728972\n",
      "Accuracy: 0.6965895727165817\n",
      "auc: 0.8156017137355263\n"
     ]
    }
   ],
   "source": [
    "hyper_pred_dev = hlm.infer(L_test[:,:])\n",
    "hyper_pred_train = hlm.infer(L_train)\n",
    "\n",
    "metrics = calculate_metrics(Y_test_numerical, hyper_pred_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "metrics = calculate_metrics(Y_train_numerical, hyper_pred_train)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941f8e1-3b12-488f-995e-e96d4b89e709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
