{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edfcec2-457b-4d0e-8542-39ad4b936a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953719da-3629-4c9d-8837-0ee49c0e9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, abstain_class=-1):\n",
    "    # Filter out samples where prediction is -1\n",
    "    valid_indices = y_pred != abstain_class\n",
    "    y_true_filtered = y_true[valid_indices]\n",
    "    y_pred_filtered = y_pred[valid_indices]\n",
    "\n",
    "    # Compute metrics\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered)\n",
    "    precision = precision_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    recall = recall_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    f1 = f1_score(y_true_filtered, y_pred_filtered, average='macro')\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "\n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcae08e6-4980-488c-a416-85e5f431bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_7b_test(image_name):\n",
    "    root_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/'\n",
    "    llava_7b_results = 'astronaut-llava 7b-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_13b_test(image_name):\n",
    "    root_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/'\n",
    "    llava_7b_results = 'astronaut-llava 13b-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def bakllava_test(image_name):\n",
    "    root_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/'\n",
    "    llava_7b_results = 'astronaut-bakllava-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_llama3_test(image_name):\n",
    "    root_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/'\n",
    "    llava_7b_results = 'astronaut-llava-llama3-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llava_phi3_test(image_name):\n",
    "    root_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/'\n",
    "    llava_7b_results = 'astronaut-llava-phi3-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def moondream_test(image_name):\n",
    "    root_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/'\n",
    "    llava_7b_results = 'astronaut-moondream-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "@labeling_function()\n",
    "def llama_3_2_vision_test(image_name):\n",
    "    root_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/'\n",
    "    llava_7b_results = 'astronaut-llama3.2-vision 11b-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def llava_34b_test(image_name):\n",
    "    root_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/'\n",
    "    llava_7b_results = 'astronaut-llava 34b-test.json'\n",
    "    path_to_llava_7b_results = os.path.join(root_path,llava_7b_results)\n",
    "    with open(path_to_llava_7b_results, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        return data[image_name]['label'] if data[image_name]['label'] is not None else -1\n",
    "    except(KeyError):\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea8dea4-b3aa-43f9-b9a5-27973b179364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(llava_7b_test('3'))\n",
    "print(llava_llama3_test('4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b2990ce-9a41-4567-8a67-7f8ec103440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2741 images in the Train set.\n",
      "There are 497 images in the test set.\n"
     ]
    }
   ],
   "source": [
    "train_data_json_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/astronaut-llava 7b-train-raw_info.json'\n",
    "test_data_json_path = '../../prompting_framework/prompting_results/agile_datasets/astronaut/astronaut-llava 34b-test-raw_info.json'\n",
    "\n",
    "with open(train_data_json_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "\n",
    "with open(test_data_json_path, 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "# Extract and pad image names, ensuring they are 5 digits long before the '.png'\n",
    "train_image_names = []\n",
    "for item in train_data:\n",
    "    train_image_names.append(item)\n",
    "\n",
    "\n",
    "test_image_names = []\n",
    "Y_test = []\n",
    "for item in test_data:\n",
    "    test_image_names.append(item)\n",
    "    Y_test.append(test_data[item][\"label\"])\n",
    "\n",
    "# with open(dev_data_json_path, 'r') as file:\n",
    "#     dev_data = json.load(file)\n",
    "    \n",
    "# dev_image_names = []\n",
    "# Y_dev = []\n",
    "# for item in dev_data:\n",
    "#     Y_dev.append(dev_data[item])\n",
    "#     dev_image_names.append(item)\n",
    "\n",
    "print(f\"There are {len(train_image_names)} images in the Train set.\")\n",
    "\n",
    "print(f\"There are {len(test_image_names)} images in the test set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7bf1b3-6719-4fee-bd77-bb948ef5d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFApplier\n",
    "\n",
    "list_of_all_the_models = [\n",
    "    'llava_13b_test',\n",
    "       'llava_7b_test',\n",
    "       'llava_34b_test',\n",
    "       'llava_llama3_test',\n",
    "       'bakllava_test',\n",
    "       'llama_3_2_vision_test',\n",
    "       'llava_phi3_test',\n",
    "       'moondream_test'\n",
    "       ]\n",
    "\n",
    "lfs = [llava_13b_test,\n",
    "       llava_7b_test,\n",
    "       llava_34b_test,\n",
    "       llava_llama3_test,\n",
    "       bakllava_test,\n",
    "       llama_3_2_vision_test,\n",
    "       llava_phi3_test,\n",
    "       moondream_test\n",
    "       ]\n",
    "\n",
    "applier = LFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35290289-1723-401c-8ee3-dc399fa3630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "497it [00:01, 416.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_test = applier.apply(test_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e19d4021-4222-4a8f-a794-86179c412693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava_13b_test</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>102</td>\n",
       "      <td>384</td>\n",
       "      <td>0.209877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_7b_test</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>135</td>\n",
       "      <td>348</td>\n",
       "      <td>0.279503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_34b_test</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.712274</td>\n",
       "      <td>0.712274</td>\n",
       "      <td>0.672032</td>\n",
       "      <td>96</td>\n",
       "      <td>258</td>\n",
       "      <td>0.271186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_llama3_test</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.975855</td>\n",
       "      <td>0.975855</td>\n",
       "      <td>0.933602</td>\n",
       "      <td>81</td>\n",
       "      <td>404</td>\n",
       "      <td>0.167010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakllava_test</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>130</td>\n",
       "      <td>356</td>\n",
       "      <td>0.267490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_3_2_vision_test</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>146</td>\n",
       "      <td>340</td>\n",
       "      <td>0.300412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llava_phi3_test</th>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>139</td>\n",
       "      <td>347</td>\n",
       "      <td>0.286008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moondream_test</th>\n",
       "      <td>7</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.975855</td>\n",
       "      <td>0.975855</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>315</td>\n",
       "      <td>170</td>\n",
       "      <td>0.649485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "llava_13b_test         0   [0, 1]  0.977867  0.977867   0.935614      102   \n",
       "llava_7b_test          1   [0, 1]  0.971831  0.971831   0.929577      135   \n",
       "llava_34b_test         2   [0, 1]  0.712274  0.712274   0.672032       96   \n",
       "llava_llama3_test      3   [0, 1]  0.975855  0.975855   0.933602       81   \n",
       "bakllava_test          4   [0, 1]  0.977867  0.977867   0.935614      130   \n",
       "llama_3_2_vision_test  5   [0, 1]  0.977867  0.977867   0.935614      146   \n",
       "llava_phi3_test        6   [0, 1]  0.977867  0.977867   0.935614      139   \n",
       "moondream_test         7   [0, 1]  0.975855  0.975855   0.935614      315   \n",
       "\n",
       "                       Incorrect  Emp. Acc.  \n",
       "llava_13b_test               384   0.209877  \n",
       "llava_7b_test                348   0.279503  \n",
       "llava_34b_test               258   0.271186  \n",
       "llava_llama3_test            404   0.167010  \n",
       "bakllava_test                356   0.267490  \n",
       "llama_3_2_vision_test        340   0.300412  \n",
       "llava_phi3_test              347   0.286008  \n",
       "moondream_test               170   0.649485  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_numerical = []\n",
    "for i in range(len(Y_test)):\n",
    "    if Y_test[i] == 'Yes':\n",
    "        Y_test_numerical.append(1)\n",
    "    elif Y_test[i] == 'No':\n",
    "        Y_test_numerical.append(0)\n",
    "\n",
    "Y_test_numerical = np.array(Y_test_numerical)\n",
    "\n",
    "LFAnalysis(L_test, lfs).lf_summary(Y_test_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47bf6d9c-4d45-4488-b050-4e3ed265bdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llava_13b_test</td>\n",
       "      <td>48</td>\n",
       "      <td>271</td>\n",
       "      <td>113</td>\n",
       "      <td>54</td>\n",
       "      <td>0.236912</td>\n",
       "      <td>0.232145</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.209756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llava_7b_test</td>\n",
       "      <td>32</td>\n",
       "      <td>284</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>0.359016</td>\n",
       "      <td>0.299742</td>\n",
       "      <td>0.279503</td>\n",
       "      <td>0.263590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llava_34b_test</td>\n",
       "      <td>51</td>\n",
       "      <td>162</td>\n",
       "      <td>96</td>\n",
       "      <td>45</td>\n",
       "      <td>0.279293</td>\n",
       "      <td>0.282165</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.270977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llava_llama3_test</td>\n",
       "      <td>37</td>\n",
       "      <td>281</td>\n",
       "      <td>123</td>\n",
       "      <td>44</td>\n",
       "      <td>0.189913</td>\n",
       "      <td>0.183317</td>\n",
       "      <td>0.167010</td>\n",
       "      <td>0.166837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bakllava_test</td>\n",
       "      <td>28</td>\n",
       "      <td>291</td>\n",
       "      <td>65</td>\n",
       "      <td>102</td>\n",
       "      <td>0.349276</td>\n",
       "      <td>0.280309</td>\n",
       "      <td>0.267490</td>\n",
       "      <td>0.250104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama_3_2_vision_test</td>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>26</td>\n",
       "      <td>141</td>\n",
       "      <td>0.429993</td>\n",
       "      <td>0.235590</td>\n",
       "      <td>0.300412</td>\n",
       "      <td>0.240974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llava_phi3_test</td>\n",
       "      <td>41</td>\n",
       "      <td>278</td>\n",
       "      <td>69</td>\n",
       "      <td>98</td>\n",
       "      <td>0.357676</td>\n",
       "      <td>0.316683</td>\n",
       "      <td>0.286008</td>\n",
       "      <td>0.276050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>moondream_test</td>\n",
       "      <td>315</td>\n",
       "      <td>3</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495283</td>\n",
       "      <td>0.326763</td>\n",
       "      <td>0.649485</td>\n",
       "      <td>0.393750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  True Negative  False Positive  False Negative  \\\n",
       "0         llava_13b_test             48             271             113   \n",
       "1          llava_7b_test             32             284              64   \n",
       "2         llava_34b_test             51             162              96   \n",
       "3      llava_llama3_test             37             281             123   \n",
       "4          bakllava_test             28             291              65   \n",
       "5  llama_3_2_vision_test              5             314              26   \n",
       "6        llava_phi3_test             41             278              69   \n",
       "7         moondream_test            315               3             167   \n",
       "\n",
       "   True Positive    Recall  Precision  Accuracy  F1 Score  \n",
       "0             54  0.236912   0.232145  0.209877  0.209756  \n",
       "1            103  0.359016   0.299742  0.279503  0.263590  \n",
       "2             45  0.279293   0.282165  0.271186  0.270977  \n",
       "3             44  0.189913   0.183317  0.167010  0.166837  \n",
       "4            102  0.349276   0.280309  0.267490  0.250104  \n",
       "5            141  0.429993   0.235590  0.300412  0.240974  \n",
       "6             98  0.357676   0.316683  0.286008  0.276050  \n",
       "7              0  0.495283   0.326763  0.649485  0.393750  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "# Example ground truth and predictions for six models\n",
    "# Replace these arrays with actual predictions from each model\n",
    "y_true = Y_test_numerical\n",
    "predictions = {}\n",
    "\n",
    "for i in range(L_test.shape[1]):\n",
    "    predictions[list_of_all_the_models[i]] = L_test[:,i]\n",
    "    \n",
    "# Create a DataFrame to store confusion matrix results and metrics\n",
    "confusion_data = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    # Confusion Matrix\n",
    "    metrics = calculate_metrics(Y_test_numerical, y_pred)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['Confusion Matrix'].ravel()\n",
    "    precision = metrics['Precision']\n",
    "    recall = metrics['Recall']\n",
    "    f1 = metrics['F1 Score']\n",
    "    accuracy = metrics['Accuracy']\n",
    "    # Append data\n",
    "    confusion_data.append([\n",
    "        model_name, tn, fp, fn, tp, \n",
    "        recall, precision, accuracy, f1\n",
    "    ])\n",
    "\n",
    "# Convert to a DataFrame for display\n",
    "confusion_df = pd.DataFrame(confusion_data, columns=[\n",
    "    'Model', 'True Negative', 'False Positive', 'False Negative', 'True Positive', \n",
    "    'Recall', 'Precision', 'Accuracy', 'F1 Score'\n",
    "])\n",
    "\n",
    "# Display the table with confusion matrix and metrics\n",
    "confusion_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc85e0-d3ce-4207-915e-7d23276063b9",
   "metadata": {},
   "source": [
    "# Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca65ba12-1f53-4e8b-9743-0dbe52a6cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(labels):\n",
    "    # Assuming the labels are categorical and using mode to find the most frequent label\n",
    "    from scipy.stats import mode\n",
    "    # Using mode along axis=1 to find the most common element across columns\n",
    "    modes = mode(labels, axis=1)\n",
    "    # modes.mode contains the most common values, reshaping to (500,) for a clean 1D array output\n",
    "    return modes.mode.reshape(-1)\n",
    "\n",
    "# Applying the majority vote function\n",
    "majority_labels = majority_vote(L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce651491-5746-4de4-8763-7c231452c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 31 288]\n",
      " [ 99  68]]\n",
      "Precision: 0.21473638720829732\n",
      "Recall: 0.2521821560640474\n",
      "F1 Score: 0.1990614367172429\n",
      "Accuracy: 0.2037037037037037\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(Y_test_numerical, majority_labels)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3e761-814f-473f-92d9-15556789ca84",
   "metadata": {},
   "source": [
    "# Snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba807c12-7238-436c-9924-6301f4bbc2c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnorkel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlabeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelModel\n\u001b[1;32m      3\u001b[0m label_model \u001b[38;5;241m=\u001b[39m LabelModel(cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m label_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mL_train\u001b[49m, Y_dev, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L_train' is not defined"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=False)\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad826a6-8a6b-4c66-8cca-038389b49066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "\n",
    "metrics = calculate_metrics(Y_dev, preds_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a2511-1c9e-4be4-9d32-e6036a30870c",
   "metadata": {},
   "source": [
    "# Hyper Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30418915-9b87-43ea-a6a1-e68d8d2ba85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/snorkel/lib/python3.12/site-packages/hyperlm/hyper_label_model.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device(self.device))\n"
     ]
    }
   ],
   "source": [
    "from hyperlm import HyperLabelModel\n",
    "hlm = HyperLabelModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce675260-6ccf-42b5-9a93-8b267897034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[ 61 262]\n",
      " [125  49]]\n",
      "Precision: 0.24275662967188744\n",
      "Recall: 0.23523184228319277\n",
      "F1 Score: 0.22087375691167238\n",
      "Accuracy: 0.22132796780684105\n"
     ]
    }
   ],
   "source": [
    "hyper_pred_dev = hlm.infer(L_test[:,:])\n",
    "# hyper_pred_train = hlm.infer(L_train)\n",
    "\n",
    "metrics = calculate_metrics(Y_test_numerical, hyper_pred_dev)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f95be-75dc-4ae7-a5f8-bb07b42d82e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
